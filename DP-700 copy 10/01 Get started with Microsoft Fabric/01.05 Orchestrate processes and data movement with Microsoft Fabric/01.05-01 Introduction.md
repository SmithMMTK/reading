
https://learn.microsoft.com/en-us/training/modules/use-data-factory-pipelines-fabric/1-introduction

_Pipelines_ คือชุดของกิจกรรมที่กำหนดลำดับขั้นตอนในการดำเนินการ โดยทั่วไปจะเริ่มจากการ _extract_ ข้อมูลจากแหล่งต้นทางหนึ่งหรือหลายแห่ง แล้วโหลด (_load_) ไปยังปลายทาง โดยมักมีการ _transform_ ข้อมูลระหว่างทางด้วย _Pipelines_ มักใช้เพื่อทำให้กระบวนการ _extract_, _transform_, และ _load_ (_ETL_) เป็นแบบอัตโนมัติ โดยข้อมูลที่นำเข้ามักเป็นข้อมูลธุรกรรมจากแหล่งข้อมูลที่ใช้ในงานปฏิบัติการ (_operational data stores_) และถูกโหลดเข้าสู่แหล่งข้อมูลสำหรับวิเคราะห์ เช่น _lakehouse_, _data warehouse_ หรือ _SQL database_

หากคุณเคยใช้งาน _Azure Data Factory_ มาก่อน _data pipelines_ ใน _Microsoft Fabric_ จะให้ประสบการณ์ที่คุ้นเคย เพราะใช้สถาปัตยกรรมเดียวกันโดยเชื่อมต่อกิจกรรมต่าง ๆ เข้าด้วยกันเพื่อกำหนดกระบวนการ ซึ่งสามารถรวมทั้งงานประมวลผลข้อมูลและตรรกะควบคุมลำดับการทำงาน (_control flow logic_) ได้ คุณสามารถสั่งรัน _pipelines_ แบบโต้ตอบได้ผ่าน _Microsoft Fabric user interface_ หรือกำหนดเวลาให้ทำงานอัตโนมัติก็ได้