
เมื่อคุณสร้าง _table_ ใน _Microsoft Fabric lakehouse_ ระบบจะสร้าง _Delta table_ ใน _metastore_ ของ _lakehouse_ และเก็บข้อมูลของ _table_ นั้นในไฟล์ _Parquet_ ที่อยู่เบื้องหลัง

ในเครื่องมือแบบ _interactive_ ส่วนใหญ่ภายใน Microsoft Fabric รายละเอียดของการเชื่อมโยงระหว่างโครงสร้าง _table_ ใน _metastore_ กับไฟล์ข้อมูลจริงจะถูกซ่อน (_abstracted_) ไว้เพื่อความสะดวก แต่ถ้าคุณทำงานกับ _Apache Spark_ ภายใน _lakehouse_ คุณจะสามารถควบคุมการสร้างและจัดการ _Delta tables_ ได้มากยิ่งขึ้น


## Creating a delta table from a dataframe

หนึ่งในวิธีที่ง่ายที่สุดในการสร้าง _Delta table_ ใน _Spark_ คือการบันทึก _dataframe_ ให้อยู่ในรูปแบบ _delta format_ ตัวอย่างโค้ด _PySpark_ ด้านล่างนี้จะแสดงการโหลดข้อมูลจากไฟล์ที่มีอยู่ แล้วบันทึกเป็น _Delta table_:


```python

# Load a file into a dataframe
df = spark.read.load('Files/mydata.csv', format='csv', header=True)

# Save the dataframe as a delta table
df.write.format("delta").saveAsTable("mytable")

```

ในโค้ดนี้ได้กำหนดให้ข้อมูลจาก _dataframe_ ถูกบันทึกเป็น _delta format_ พร้อมตั้งชื่อตารางว่า "mytable" ข้อมูลของตารางจะถูกจัดเก็บเป็นไฟล์ _Parquet_ (ไม่ว่าต้นทางจะเป็นไฟล์รูปแบบใดก็ตาม) ภายในพื้นที่จัดเก็บ _Tables_ ของ _lakehouse_ และมีโฟลเดอร์ _delta_log_ ที่เก็บ _transaction logs_ ของตารางนี้

เมื่อตารางถูกสร้างเรียบร้อยแล้ว จะสามารถมองเห็นได้ในโฟลเดอร์ _Tables_ ของ _lakehouse_ บน _Data explorer pane_

## _Managed_ vs _external_ tables

ในตัวอย่างก่อนหน้า _dataframe_ ถูกบันทึกเป็น _managed table_ หมายความว่า ทั้งโครงสร้างตารางใน _metastore_ และไฟล์ข้อมูลที่อยู่เบื้องหลังจะถูกจัดการโดย _Spark runtime_ ของ _Fabric lakehouse_ ถ้ามีการลบตารางนี้ ไฟล์ข้อมูลที่อยู่ในโฟลเดอร์ **Tables** ก็จะถูกลบไปด้วย

นอกจากนี้ คุณยังสามารถสร้างตารางแบบ _external table_ ได้ด้วย ซึ่งหมายถึงการกำหนด _table_ ใน _metastore_ ให้เชื่อมโยงกับตำแหน่งเก็บไฟล์อื่นที่ไม่ใช่โฟลเดอร์ **Tables**

ตัวอย่างเช่น โค้ดด้านล่างนี้เป็นการสร้าง _external table_ ที่เก็บข้อมูลไว้ในโฟลเดอร์ใต้พื้นที่ **Files** ของ _lakehouse_:

```python
df.write.format("delta").saveAsTable("myexternaltable", path="Files/myexternaltable")
```

ในตัวอย่างนี้ โครงสร้างของตารางจะถูกสร้างไว้ใน _metastore_ (ดังนั้นตารางจะปรากฏอยู่ในหน้า **Tables** ของ _lakehouse UI_) แต่ไฟล์ข้อมูล _Parquet_ และไฟล์ _JSON log_ ของตารางนี้จะถูกเก็บไว้ในพื้นที่ **Files** และสามารถดูได้จากโหนด **Files** ในแถบ **Lakehouse explorer**

คุณยังสามารถระบุ _path_ แบบเต็ม (_fully qualified path_) สำหรับตำแหน่งจัดเก็บข้อมูลได้ เช่น:

```python
df.write.format("delta").saveAsTable("myexternaltable", path="abfss://my_store_url..../myexternaltable")
```

การลบ _external table_ ออกจาก _lakehouse metastore_ **จะไม่ลบ** ไฟล์ข้อมูลที่เชื่อมโยงกับตารางนั้น กล่าวคือ แม้ตารางจะถูกลบออกจากรายการใน **Tables** แล้ว แต่ไฟล์ _Parquet_ และ _delta log_ ที่อยู่ในพื้นที่ **Files** หรือ _path_ ภายนอกที่กำหนดไว้ ยังคงอยู่เหมือนเดิม

## Creating table metadata

ถึงแม้ว่าเรามักจะ _สร้างตาราง_ จากข้อมูลที่มีอยู่แล้วใน _dataframe_ แต่ก็มีหลายสถานการณ์ที่เราต้องการสร้าง _table definition_ ใน _metastore_ ล่วงหน้า โดยที่ข้อมูลจะถูกเติมเข้ามาภายหลังจากวิธีอื่น ๆ เราสามารถทำแบบนี้ได้หลายวิธี 

### Use the _DeltaTableBuilder_ API

_API_ ที่ชื่อว่า **DeltaTableBuilder** ช่วยให้คุณสามารถเขียนโค้ด _Spark_ เพื่อสร้าง _table_ ตามที่คุณกำหนดเองได้ เช่น ตัวอย่างโค้ดด้านล่างนี้ใช้ในการสร้าง _table_ โดยระบุชื่อและ _columns_ ที่ต้องการอย่างชัดเจน


```python
from delta.tables import *

DeltaTable.create(spark) \
  .tableName("products") \
  .addColumn("Productid", "INT") \
  .addColumn("ProductName", "STRING") \
  .addColumn("Category", "STRING") \
  .addColumn("Price", "FLOAT") \
  .execute()
```

### Use Spark SQL

คุณยังสามารถสร้าง _delta table_ ได้โดยใช้คำสั่ง _Spark SQL_ อย่าง `CREATE TABLE` ตามตัวอย่างนี้


```sql
%%sql

CREATE TABLE salesorders
(
    Orderid INT NOT NULL,
    OrderDate TIMESTAMP NOT NULL,
    CustomerName STRING,
    SalesTotal FLOAT NOT NULL
)
USING DELTA
```

ตัวอย่างก่อนหน้านี้เป็นการสร้าง _managed table_ แต่ถ้าคุณต้องการสร้าง _external table_ ก็สามารถทำได้โดยการระบุพารามิเตอร์ `LOCATION` ดังตัวอย่างนี้

```sql
%%sql

CREATE TABLE MyExternalTable
USING DELTA
LOCATION 'Files/mydata'
```


เมื่อคุณสร้าง _external table_ โครงสร้างของ _schema_ จะถูกกำหนดจากไฟล์ _Parquet_ ที่อยู่ในตำแหน่งที่คุณระบุไว้ใน `LOCATION` วิธีนี้มีประโยชน์มากในกรณีที่คุณต้องการสร้าง _table definition_ ที่อ้างอิงกับข้อมูลที่ถูกบันทึกไว้ใน _delta format_ อยู่แล้ว หรือในโฟลเดอร์ที่คุณวางแผนว่าจะนำเข้าข้อมูลในรูปแบบ _delta format_ ในอนาคต

## Saving data in delta format

ถึงตอนนี้คุณได้เห็นแล้วว่าเราสามารถ
1. _save dataframe เป็น delta table_ (สร้างทั้ง _schema_ ใน _metastore_ และไฟล์ข้อมูลใน _delta format_)  
2. _สร้าง table definition_ อย่างเดียว (สร้างเฉพาะ _schema_ โดยไม่บันทึกไฟล์ข้อมูล)
และยังมีทางเลือกที่สาม คือการ _บันทึกข้อมูลใน delta format_ โดยไม่สร้าง _table definition_ ใน _metastore_ วิธีนี้เหมาะในกรณีที่คุณต้องการเก็บผลลัพธ์จากการแปลงข้อมูลใน _Spark_ ไว้ในรูปแบบไฟล์ ซึ่งสามารถนำไปใช้งานต่อภายหลังได้ เช่น นำมา _overlay_ เป็น _table_ หรือเรียกใช้โดยตรงผ่าน _Delta Lake API_

ตัวอย่างเช่น โค้ด _PySpark_ ด้านล่างนี้จะบันทึก _dataframe_ ไปยังโฟลเดอร์ใหม่ในรูปแบบ _delta_:

```python
delta_path = "Files/mydatatable"
df.write.format("delta").save(delta_path)
```

ไฟล์ _delta_ จะถูกบันทึกในรูปแบบ _Parquet_ ที่ตำแหน่งที่คุณระบุไว้ และจะมีโฟลเดอร์ชื่อ **_delta_log** ซึ่งใช้เก็บ _transaction log files_ ที่บันทึกการเปลี่ยนแปลงทั้งหมดของข้อมูล เช่น การอัปเดตที่ทำกับ _external tables_ หรือผ่านการใช้งาน _Delta Lake API_

ถ้าคุณต้องการ _เขียนทับ_ ข้อมูลในโฟลเดอร์ที่มีอยู่แล้วด้วยข้อมูลจาก _dataframe_ สามารถใช้โหมด **overwrite** ได้ ดังตัวอย่างนี้:

```python
new_df.write.format("delta").mode("overwrite").save(delta_path)
```

คุณสามารถเพิ่มแถว (_rows_) จาก _dataframe_ เข้าไปในโฟลเดอร์ที่มีอยู่แล้วได้ โดยใช้โหมด **_append_**

```python
new_rows_df.write.format("delta").mode("append").save(delta_path)
```

_เคล็ดลับ_: ถ้าคุณใช้เทคนิคที่อธิบายไว้ที่นี่ในการบันทึก _dataframe_ ไปยังตำแหน่ง **Tables** ใน _lakehouse_ ระบบ Microsoft Fabric จะใช้ความสามารถของ _automatic table discovery_ เพื่อสร้าง _table metadata_ ให้โดยอัตโนมัติใน _metastore_
