
ข้อมูลคือหัวใจของ _data science_ โดยเฉพาะเมื่อต้องการฝึก _machine learning model_ เพื่อบรรลุเป้าหมายด้าน _artificial intelligence_ โดยทั่วไปแล้ว โมเดลจะมีประสิทธิภาพดีขึ้นเมื่อมีขนาดของ _training dataset_ ที่ใหญ่ขึ้น แต่ไม่ใช่แค่ปริมาณเท่านั้นที่สำคัญ คุณภาพของข้อมูลก็มีความสำคัญไม่แพ้กัน

เพื่อรับประกันทั้งคุณภาพและปริมาณของข้อมูล การใช้ _Microsoft Fabric_ ซึ่งมี _data ingestion_ และ _processing engine_ ที่ทรงพลังจึงเป็นทางเลือกที่คุ้มค่า คุณสามารถเลือกใช้วิธีแบบ _low-code_ หรือ _code-first_ เพื่อสร้าง _pipeline_ สำหรับ _ingestion_, _exploration_ และ _transformation_ ได้ตามความถนัด

## Ingest your data into Microsoft Fabric

หากต้องการทำงานกับข้อมูลใน _Microsoft Fabric_ ขั้นตอนแรกคือต้อง _ingest_ ข้อมูลเข้ามาก่อน คุณสามารถ _ingest_ ข้อมูลจากหลายแหล่ง ไม่ว่าจะเป็นแหล่งข้อมูลภายในเครื่องหรือบนคลาวด์ เช่น คุณสามารถ _ingest_ ข้อมูลจากไฟล์ _CSV_ ที่อยู่ในเครื่องของคุณ หรือจาก _Azure Data Lake Storage (Gen2)_

_Tip_

ศึกษาวิธี [ingest และ orchestrate ข้อมูลจากหลากหลายแหล่งด้วย Microsoft Fabric](https://learn.microsoft.com/en-us/training/paths/ingest-data-with-microsoft-fabric/)

หลังจากเชื่อมต่อกับแหล่งข้อมูลแล้ว คุณสามารถบันทึกข้อมูลลงใน **lakehouse** ของ _Microsoft Fabric_ ซึ่งเป็นศูนย์กลางสำหรับจัดเก็บไฟล์แบบ _structured_, _semi-structured_, และ _unstructured_ จากนั้นคุณสามารถเข้าถึง _lakehouse_ ได้ง่ายเมื่อใดก็ตามที่ต้องการนำข้อมูลไปใช้สำรวจหรือแปลง

## Explore and transform your data

ในฐานะ _data scientist_ คุณอาจคุ้นเคยกับการเขียนและรันโค้ดใน **notebooks** ซึ่ง _Microsoft Fabric_ ก็มี _notebook_ ให้ใช้งานที่มาพร้อม _Spark compute_ ที่คุ้นเคย

**Apache Spark** เป็น _open source parallel processing framework_ สำหรับประมวลผลและวิเคราะห์ข้อมูลขนาดใหญ่

_notebook_ จะเชื่อมต่อกับ _Spark compute_ โดยอัตโนมัติ เมื่อคุณรันคำสั่งในเซลล์ครั้งแรก ระบบจะเริ่ม _Spark session_ ใหม่ ซึ่งจะยังคงอยู่ในขณะที่คุณรันเซลล์ถัด ๆ ไป _Spark session_ จะหยุดโดยอัตโนมัติหลังไม่มีการใช้งานในช่วงเวลาหนึ่งเพื่อประหยัดค่าใช้จ่าย หรือคุณสามารถหยุดเองได้เช่นกัน

คุณสามารถเลือกภาษาในการทำงานใน _notebook_ ได้ตามที่ถนัด โดยสำหรับงาน _data science_ มักจะใช้ _PySpark (Python)_ หรือ _SparkR (R)_

![Screenshot of a notebook in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/get-started-data-science-fabric/media/notebooks.png)

ภายใน _notebook_ คุณสามารถสำรวจข้อมูลของคุณได้โดยใช้ไลบรารีที่คุณถนัด หรือใช้ _visualization_ ที่มีให้ในระบบ และหากต้องการ คุณสามารถแปลงข้อมูลและเขียนข้อมูลที่แปลงแล้วกลับเข้าไปยัง _lakehouse_ ได้

### Prepare your data with the Data Wrangler

เพื่อช่วยให้คุณสำรวจและแปลงข้อมูลได้รวดเร็วยิ่งขึ้น _Microsoft Fabric_ มีเครื่องมือที่ใช้งานง่ายชื่อว่า **Data Wrangler**

เมื่อคุณเปิด _Data Wrangler_ คุณจะเห็นภาพรวมของข้อมูลที่กำลังทำงานอยู่ พร้อมทั้งสถิติสรุปที่ช่วยให้คุณตรวจพบปัญหา เช่น _missing values_

คุณสามารถเลือกใช้เครื่องมือ _data-cleaning_ ที่มีมาให้ในระบบ เมื่อเลือกคำสั่ง ระบบจะแสดงตัวอย่างผลลัพธ์พร้อมทั้งโค้ดที่เกี่ยวข้องโดยอัตโนมัติ เมื่อคุณเลือกคำสั่งทั้งหมดเรียบร้อยแล้ว คุณสามารถส่งออกการแปลงข้อมูลเป็นโค้ดและนำไปใช้กับข้อมูลของคุณได้ทันที