
ข้อมูลทั้งหมดที่เราได้สำรวจมาจนถึงตอนนี้เป็นข้อมูลแบบ _static_ ที่อยู่ในไฟล์ อย่างไรก็ตาม ในหลาย ๆ กรณีของการวิเคราะห์ข้อมูล เราจำเป็นต้องทำงานกับข้อมูลแบบ _streaming_ ซึ่งต้องประมวลผลให้ใกล้เคียงกับเวลาจริง (_near real-time_) เช่น อาจต้องเก็บค่าที่อุปกรณ์ _IoT_ ส่งออกมาแบบต่อเนื่องและบันทึกไว้ในตารางทันทีที่เกิดขึ้น

_Spark_ สามารถประมวลผลข้อมูลแบบ _batch_ และ _streaming_ ได้เหมือนกัน โดยใช้ _API_ แบบเดียวกัน ซึ่งหมายความว่าเราสามารถใช้ _Spark_ ในการประมวลผลข้อมูล _streaming_ แบบ _real-time_ ได้เลย โดยไม่ต้องเปลี่ยนแปลงวิธีเขียนโค้ดมากนัก

## Spark Structured Streaming

โซลูชันสำหรับ _stream processing_ ทั่วไปจะประกอบด้วย 3 ขั้นตอนหลัก:

1. อ่านข้อมูลจาก _streaming source_ อย่างต่อเนื่อง
2. (อาจมี) การประมวลผลข้อมูล เช่น เลือกเฉพาะบางฟิลด์, รวมค่า (_aggregate_), จัดกลุ่ม (_group_) หรือปรับแต่งข้อมูลในรูปแบบอื่น
3. เขียนผลลัพธ์ลง _sink_
    
_Spark_ รองรับการทำงานกับข้อมูล _streaming_ โดยตรงผ่าน _Spark Structured Streaming_ ซึ่งเป็น _API_ ที่อิงจาก _boundless dataframe_ ที่สามารถจับข้อมูล _streaming_ มาประมวลผลได้

_dataframe_ ของ _Spark Structured Streaming_ สามารถอ่านข้อมูลจากแหล่ง _streaming_ ได้หลายประเภท เช่น
- พอร์ตเครือข่าย (_network ports_) 
- บริการจัดการข้อความแบบ _real-time_ เช่น _Azure Event Hubs_ หรือ _Kafka_
- ตำแหน่งบนระบบไฟล์ (_file system locations_)

_เคล็ดลับ_

ถ้าต้องการเรียนรู้เพิ่มเติมเกี่ยวกับ _Spark Structured Streaming_ สามารถดูได้ที่หัวข้อ _Structured Streaming Programming Guide_ ในเอกสารของ _Spark_

## Streaming with Delta tables

คุณสามารถใช้ _Delta table_ เป็นทั้ง _source_ และ _sink_ สำหรับ _Spark Structured Streaming_ ได้ ตัวอย่างเช่น คุณอาจจะรับข้อมูล _real-time_ จากอุปกรณ์ _IoT_ แล้วเขียนข้อมูลนั้นลงใน _Delta table_ โดยตรงในฐานะ _sink_ หลังจากนั้นคุณสามารถรันคำสั่ง _query_ กับตารางนั้นเพื่อดูข้อมูล _streamed_ ล่าสุดได้
 
หรืออีกทางหนึ่ง คุณอาจใช้ _Delta table_ เป็น _streaming source_ เพื่อให้สามารถรายงานผลได้แบบ _near real-time_ เมื่อมีข้อมูลใหม่ถูกเพิ่มเข้ามาในตาราง

### Using a Delta table as a streaming source

ในตัวอย่าง _PySpark_ ต่อไปนี้ เราจะสร้าง _Delta table_ เพื่อเก็บรายละเอียดของคำสั่งซื้อสินค้าผ่านอินเทอร์เน็ต (_Internet sales orders_)

```sql
%%sql
CREATE TABLE orders_in
(
        OrderID INT,
        OrderDate DATE,
        Customer STRING,
        Product STRING,
        Quantity INT,
        Price DECIMAL
)
USING DELTA;
```

ข้อมูล _streaming_ สมมุติของคำสั่งซื้อทางอินเทอร์เน็ตจะถูกแทรกเข้าไปในตาราง _orders_in_

```sql
%%sql
INSERT INTO orders_in (OrderID, OrderDate, Customer, Product, Quantity, Price)
VALUES
    (3001, '2024-09-01', 'Yang', 'Road Bike Red', 1, 1200),
    (3002, '2024-09-01', 'Carlson', 'Mountain Bike Silver', 1, 1500),
    (3003, '2024-09-02', 'Wilson', 'Road Bike Yellow', 2, 1350),
    (3004, '2024-09-02', 'Yang', 'Road Front Wheel', 1, 115),
    (3005, '2024-09-02', 'Rai', 'Mountain Bike Black', 1, NULL);
```

เพื่อยืนยันผล คุณสามารถอ่านและแสดงข้อมูลจากตาราง _orders_in_ ได้

```python
# Read and display the input table
df = spark.read.format("delta").table("orders_in")

display(df)
```

จากนั้นข้อมูลจะถูกโหลดเข้าสู่ _streaming DataFrame_ จาก _Delta table_

```python
# Load a streaming DataFrame from the Delta table
stream_df = spark.readStream.format("delta") \
    .option("ignoreChanges", "true") \
    .table("orders_in")
```

_หมายเหตุ_

เมื่อใช้ _Delta table_ เป็น _streaming source_ จะสามารถรองรับได้เฉพาะกรณีที่เป็น _append operations_ เท่านั้น
ถ้ามีการแก้ไขข้อมูล (_modifications_) จะทำให้เกิดข้อผิดพลาด เว้นแต่คุณจะระบุตัวเลือก _ignoreChanges_ หรือ _ignoreDeletes_ เพื่อให้ _Spark_ มองข้ามการเปลี่ยนแปลงเหล่านั้น

คุณสามารถตรวจสอบได้ว่า _DataFrame_ นั้นเป็น _streaming_ อยู่หรือไม่ โดยใช้คุณสมบัติ _isStreaming_ ซึ่งควรจะคืนค่าเป็น True หากกำลังทำงานกับ _streaming_ อยู่

```python
# Verify that the stream is streaming
stream_df.isStreaming
```

### Transform the data stream

หลังจากโหลดข้อมูลจาก _Delta table_ เข้ามาใน _streaming DataFrame_ แล้ว เราสามารถใช้ _Spark Structured Streaming API_ เพื่อประมวลผลข้อมูลแบบสตรีมได้ เช่น เราอาจนับจำนวนออเดอร์ที่ถูกสั่งในแต่ละนาที แล้วส่งผลรวมนี้ไปยังกระบวนการถัดไป เพื่อแสดงผลแบบใกล้เคียง _real-time_

ในตัวอย่างนี้ จะมีการกรองแถวที่มีค่า _NULL_ ในคอลัมน์ _Price_ ออกไป และมีการเพิ่มคอลัมน์ใหม่ชื่อว่า _IsBike_ และ _Total_

```python
from pyspark.sql.functions import col, expr

transformed_df = stream_df.filter(col("Price").isNotNull()) \
    .withColumn('IsBike', expr("INSTR(Product, 'Bike') > 0").cast('int')) \
    .withColumn('Total', expr("Quantity * Price").cast('decimal'))
```

### Using a Delta table as a streaming sink

จากนั้น _data stream_ จะถูกเขียนลงใน _Delta table_ 

```python
# Write the stream to a delta table
output_table_path = 'Tables/orders_processed'
checkpointpath = 'Files/delta/checkpoint'
deltastream = transformed_df.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(output_table_path)

print("Streaming to orders_processed...")
```

_Note_  
ตัวเลือก _checkpointLocation_ ใช้สำหรับเขียนไฟล์ _checkpoint_ ซึ่งมีหน้าที่ติดตามสถานะของกระบวนการประมวลผล _stream_ ไฟล์นี้จะช่วยให้ระบบสามารถ _recover_ หรือกู้คืนการทำงานต่อได้จากจุดที่หยุดไป หากเกิดความล้มเหลวระหว่างประมวลผล

หลังจากที่เริ่มกระบวนการ _streaming_ แล้ว คุณสามารถสั่ง _query_ ตาราง _Delta Lake_ เพื่อตรวจสอบว่ามีข้อมูลอะไรบ้างใน _output table_ ได้ โดยอาจมีการหน่วงเล็กน้อยก่อนที่คุณจะสามารถ _query_ ตารางนั้นได้

```sql
%%sql
SELECT *
    FROM orders_processed
    ORDER BY OrderID;
```

ในผลลัพธ์ของ _query_ นี้ คำสั่งซื้อหมายเลข 3005 จะไม่ถูกแสดง เนื่องจากมีค่า _NULL_ อยู่ในคอลัมน์ _Price_ และจะมีสองคอลัมน์ที่ถูกเพิ่มเข้ามาระหว่างการแปลงข้อมูล ได้แก่ _IsBike_ และ _Total_ แสดงให้เห็นในตารางด้วย

|OrderID|OrderDate|Customer|Product|Quantity|Price|IsBike|Total|
|---|---|---|---|---|---|---|---|
|3001|2023-09-01|Yang|Road Bike Red|1|1200|1|1200|
|3002|2023-09-01|Carlson|Mountain Bike Silver|1|1500|1|1500|
|3003|2023-09-02|Wilson|Road Bike Yellow|2|1350|1|2700|
|3004|2023-09-02|Yang|Road Front Wheel|1|115|0|115|

เมื่อเสร็จสิ้นแล้ว ให้หยุดการ _streaming data_ เพื่อหลีกเลี่ยงค่าใช้จ่ายจากการประมวลผลที่ไม่จำเป็น โดยใช้เมธอด stop() ดังนี้:

```python
# หยุด streaming data เพื่อหลีกเลี่ยงค่าใช้จ่ายจากการประมวลผลที่เกินจำเป็น
deltastream.stop()
```

_Tip_: หากต้องการข้อมูลเพิ่มเติมเกี่ยวกับการใช้ _Delta tables_ กับ _streaming data_ โปรดดูหัวข้อ _Table streaming reads and writes_ ในเอกสารของ _Delta Lake_

