Classification เป็นเทคนิค machine learning แบบ _supervised_ เช่นเดียวกับ regression โดยมีขั้นตอนการ _training_, _validating_ และ _evaluating_ โมเดลแบบวนซ้ำ แตกต่างจาก regression ที่ทำนายค่าตัวเลข อัลกอริทึมของ classification จะคำนวณค่า _probability_ สำหรับการจัดกลุ่มข้อมูล และใช้ค่าที่ได้มาเปรียบเทียบกับค่าจริงเพื่อประเมินประสิทธิภาพของโมเดล

**_Binary classification_** ใช้สำหรับฝึกโมเดลให้ทำนายผลลัพธ์ที่มีสองค่าทางเลือก เช่น **_true_** หรือ **_false_** โดยทั่วไปข้อมูลจะประกอบด้วยหลายค่า **_x_** (features) และค่าผลลัพธ์ **_y_** ซึ่งมีค่าเป็น **1** หรือ **0**

### Example - binary classification

| **Blood glucose (x)** | **Diabetic? (y)** |
| --------------------- | ----------------- |
| 67                    | 0                 |
| 103                   | 1                 |
| 114                   | 1                 |
| 72                    | 0                 |
| 116                   | 1                 |
| 65                    | 0                 |

### Training a binary classification model

การฝึกโมเดลจะใช้อัลกอริทึมสร้างฟังก์ชันที่คำนวณ “ความน่าจะเป็น” ที่ค่าปลายทาง (เช่น ผู้ป่วยเป็นเบาหวาน) จะเป็นจริง ความน่าจะเป็นอยู่ในช่วง 0.0 ถึง 1.0 โดยผลรวมของทุกคลาสต้องรวมกันได้ 1.0 (เช่น ถ้ามีโอกาสเป็นเบาหวาน 0.7 ก็แปลว่าโอกาสไม่เป็นคือ 0.3)

มีอัลกอริทึมหลายแบบสำหรับ binary classification เช่น **logistic regression** ซึ่งใช้ฟังก์ชันรูปตัว S (sigmoid) เพื่อให้ออกค่าระหว่าง 0.0 ถึง 1.0

![Diagram of a logistic function.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/sigmoid-plot.png)

> **Note:**
> 
> แม้ชื่อจะมีคำว่า regression แต่ใน machine learning logistic regression ใช้สำหรับ classification ไม่ใช่การทำนายค่าตัวเลข จุดสำคัญคือมันใช้ฟังก์ชันแบบ logistic ซึ่งมีลักษณะเป็นเส้นโค้งรูปตัว S (sigmoid) ที่ให้ค่าผลลัพธ์อยู่ระหว่าง 0.0 ถึง 1.0 ซึ่งเหมาะกับการจำแนกกรณีที่มีแค่สองผลลัพธ์ (binary classification) เช่น ใช่/ไม่ใช่ หรือ จริง/เท็จ


ฟังก์ชันที่ได้จากอัลกอริทึมจะใช้เพื่อบอกความน่าจะเป็นที่ค่า **y** จะเป็นจริง (y = 1) สำหรับค่าของ **x** ที่กำหนด โดยสามารถแสดงในรูปสมการทางคณิตศาสตร์ได้ว่า:

_**f(x) = P(y = 1 | x)**_

ในข้อมูลฝึกจำนวน 6 ตัวอย่าง มี 3 ตัวอย่างที่ทราบแน่ชัดว่า **_y_** เป็น _true_ จึงให้ค่า _y = 1_ มีความน่าจะเป็นเป็น **1.0** ส่วนอีก 3 ตัวอย่างที่ทราบว่า **_y_** เป็น _false_ จะให้ความน่าจะเป็นที่ _y = 1_ เป็น **0.0**

เส้นโค้งรูปตัว S (sigmoid) ใช้อธิบายการกระจายของความน่าจะเป็น โดยสามารถนำค่า **_x_** มาพล็อตลงบนกราฟเพื่อดูว่าความน่าจะเป็นที่ **_y_** จะเป็น **1** มีเท่าไร

ในภาพยังแสดงเส้นแนวนอนที่เป็น _threshold_ ซึ่งใช้กำหนดจุดตัดสินใจของโมเดล ค่า threshold โดยทั่วไปอยู่ที่ค่ากลางของ **_y_** คือ _P(y) = 0.5_ ถ้าค่าความน่าจะเป็นเท่ากับหรือมากกว่า 0.5 โมเดลจะทำนายว่า **_y = 1_** (_true_) แต่ถ้าต่ำกว่า 0.5 จะทำนายว่า **_y = 0_** (_false_)

เช่น ถ้าผู้ป่วยมีระดับน้ำตาลในเลือดเท่ากับ 90 และฟังก์ชันให้ค่าความน่าจะเป็นออกมาเป็น 0.9 ซึ่งมากกว่า threshold ที่ 0.5 โมเดลก็จะทำนายว่า **_y = 1_** หรือผู้ป่วยมีแนวโน้มเป็นเบาหวาน

### Evaluating a binary classification model

เช่นเดียวกับ regression ในการฝึกโมเดล binary classification จะมีการกันข้อมูลบางส่วนแบบสุ่มไว้สำหรับใช้ในการ _validation_ เพื่อตรวจสอบว่าโมเดลที่ฝึกมาแล้วสามารถทำนายข้อมูลใหม่ได้แม่นยำหรือไม่

สมมุติว่าเรากันข้อมูลบางชุดไว้เพื่อตรวจสอบประสิทธิภาพของโมเดลที่ใช้จำแนกว่าผู้ป่วยเป็นเบาหวานหรือไม่ (diabetes classifier)

| Blood glucose (x) | Diabetic? (y) |
| ----------------- | ------------- |
| 66                | 0             |
| 107               | 1             |
| 112               | 1             |
| 71                | 0             |
| 87                | 1             |
| 89                | 1             |

เมื่อเรานำฟังก์ชันโลจิสติก (logistic function) ที่ได้จากการฝึกโมเดลมาใช้กับค่าต่าง ๆ ของ **_x_** จะได้กราฟที่แสดงความน่าจะเป็นของ **_y = 1_** สำหรับแต่ละค่า _x_ ซึ่งมีลักษณะเป็นเส้นโค้งรูปตัว S โดยค่าในแนวตั้ง (แกน y) คือค่าความน่าจะเป็นที่โมเดลทำนายว่า _y_ เป็น _true_ หรือ **1**

![Diagram of predicted labels on a sigmoid curve.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/classification-predictions.png)

โดยโมเดลจะใช้ค่าความน่าจะเป็นที่ได้จากฟังก์ชันมาตัดสินว่าแต่ละตัวอย่างควรมีค่าทำนายเป็น **1** หรือ **0** ขึ้นอยู่กับว่าอยู่เหนือหรือใต้ค่า _threshold_ (โดยทั่วไปคือ 0.5) จากนั้นเราจะนำผลลัพธ์ที่โมเดลทำนาย (_ŷ_) มาเปรียบเทียบกับค่าจริง (_y_) เพื่อดูว่าโมเดลทำนายได้ถูกต้องหรือไม่ในแต่ละกรณี

| Blood glucose (x) | Predicted diabetes diagnosis (ŷ) | Actual diabetes diagnosis (y) |
| ----------------- | -------------------------------- | ----------------------------- |
| 66                | 0                                | 0                             |
| 107               | 1                                | 1                             |
| 112               | 1                                | 1                             |
| 71                | 0                                | 0                             |
| 87                | **0**                            | **1**                         |
| 89                | 1                                | 1                             |

### Binary classification evaluation metrics

ขั้นตอนแรกของการประเมินโมเดล binary classification คือการสร้าง **confusion matrix** ซึ่งเป็นตารางที่แสดงจำนวนครั้งที่โมเดลทำนายถูกหรือผิดสำหรับแต่ละคลาส โดยแบ่งออกเป็น 4 กรณี:

![Diagram of a confusion matrix.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/binary-confusion-matrix.png)

ภาพนี้เรียกว่า **confusion matrix** ซึ่งแสดงจำนวนผลการทำนายตามแต่ละกรณี โดยแยกเป็น:

- ŷ = 0 และ y = 0: **True negatives (TN)**
- ŷ = 1 และ y = 1: **True positives (TP)**
- ŷ = 0 และ y = 1: **False negatives (FN)** > **“คนป่วยจริง ๆ แต่โมเดลบอกว่าไม่ป่วย”**
- ŷ = 1 และ y = 0: **False positives (FP)** > **“คนไม่ป่วย แต่โมเดลบอกว่าป่วย”**

การจัดเรียง confusion matrix จะวาง **ค่าทำนายถูก (true)** ไว้ตามแนวทแยงจากซ้ายบนไปขวาล่าง หากโมเดลทำนายได้ดี จะเห็นแนวเฉดสีเข้มตามเส้นทแยงนี้ เนื่องจากมีจำนวนการทำนายถูกต้องมากในแต่ละคลาส

---
#### Accuracy

เมตริกที่ง่ายที่สุดที่ได้จาก confusion matrix คือ *accuracy* หรืออัตราความถูกต้อง คำนวณจากสูตร:  
(TN + TP) ÷ (TN + FN + FP + TP)

ในตัวอย่างโมเดลทำนายโรคเบาหวาน:  
(2 + 3) ÷ (2 + 1 + 0 + 3) = 5 ÷ 6 = **0.83**

แสดงว่าโมเดลทำนายถูกต้อง 83% ของกรณีในข้อมูล validation

แม้ *accuracy* จะดูดี แต่ก็อาจทำให้เข้าใจผิดได้ เช่น ถ้าประชากรมีผู้เป็นเบาหวานเพียง 11% โมเดลที่ทำนายว่า “ไม่เป็น” ตลอดจะได้ accuracy ถึง 89% ทั้งที่ไม่มีความสามารถแยกแยะจริง  
ดังนั้นเราควรดูเมตริกอื่นร่วมด้วย เพื่อวัดความสามารถของโมเดลให้ชัดเจนขึ้น

---
#### Recall > **“ในบรรดาคนที่เป็นจริง ๆ โมเดลจับได้ถูกกี่คน?”**

*Recall* เป็นเมตริกที่ใช้วัดสัดส่วนของผู้ป่วยที่เป็นจริง แล้วโมเดลสามารถตรวจจับได้ถูกต้อง  
พูดง่าย ๆ คือ ในบรรดาผู้ที่เป็นเบาหวานจริง ๆ โมเดลทำนายได้ถูกต้องกี่คน

สูตรของ recall คือ:  
TP ÷ (TP + FN)

สำหรับตัวอย่างโรคเบาหวาน:  
3 ÷ (3 + 1) = 3 ÷ 4 = **0.75**

แสดงว่าโมเดลสามารถระบุผู้ที่เป็นเบาหวานได้ถูกต้อง **75%**

---
#### Precision (False Positive focus) > **“โมเดลบอกว่าใครป่วย แล้วในนั้นมีกี่คนที่ป่วยจริง”**

*Precision* เป็นเมตริกที่คล้ายกับ recall แต่ใช้วัดสัดส่วนของผู้ที่โมเดลทำนายว่าเป็นบวก แล้ว *เป็นบวกจริง*  
พูดง่าย ๆ คือ ในบรรดาผู้ที่โมเดลทำนายว่าเป็นเบาหวาน มีเท่าไหร่ที่เป็นจริง

สูตรของ precision คือ:  
TP ÷ (TP + FP)

สำหรับตัวอย่างโรคเบาหวาน:  
3 ÷ (3 + 0) = 3 ÷ 3 = **1.0**

แสดงว่า 100% ของผู้ที่โมเดลทำนายว่าเป็นเบาหวาน ล้วนเป็นเบาหวานจริง

---
#### F1-score > **ความแม่นยำโดยรวมของโมเดล แบบสมดุลทั้ง “ไม่ฟ้องเกิน” และ “ไม่พลาดของจริง”**

*F1-score* เป็นเมตริกแบบองค์รวมที่รวมทั้ง *precision* และ *recall* เข้าด้วยกัน  
เหมาะสำหรับกรณีที่ต้องการสมดุลระหว่างการทำนายค่าบวกให้แม่น และไม่พลาดค่าบวกที่มีอยู่จริง

สูตรของ F1-score คือ:  
(2 × Precision × Recall) ÷ (Precision + Recall)

สำหรับตัวอย่างโรคเบาหวาน:  
(2 × 1.0 × 0.75) ÷ (1.0 + 0.75) = 1.5 ÷ 1.75 = **0.86**

F1-score จึงสรุปว่าโมเดลมีความแม่นยำโดยรวมอยู่ที่ **86%**

---
##### สรุป
| **เมตริก**    | **ใช้เมื่อ…**                          | **สูตร**                                        |
| ------------- | -------------------------------------- | ----------------------------------------------- |
| **Accuracy**  | ข้อมูลสมดุล และอยากดูภาพรวม            | (TP + TN) ÷ (TP + TN + FP + FN)                 |
| **Recall**    | กลัวพลาดผู้ป่วย (False Negative สำคัญ) | TP ÷ (TP + FN)                                  |
| **Precision** | กลัวฟ้องเกิน (False Positive สำคัญ)    | TP ÷ (TP + FP)                                  |
| **F1-score**  | ต้องการสมดุลทั้งสองด้าน                | (2 × Precision × Recall) ÷ (Precision + Recall) |

#### Area Under the Curve (AUC)

อีกชื่อหนึ่งของ *recall* คือ *True Positive Rate (TPR)* > **ในบรรดาคนที่ “เป็นจริง” → โมเดลจับได้ถูกกี่คน** และมีเมตริกที่สัมพันธ์กันชื่อว่า *False Positive Rate (FPR)*  > **> ในบรรดาคนที่ “ไม่เป็นจริง” → โมเดลหลงทำนายว่าป่วยกี่คน**
ซึ่งคำนวณจากสูตร: FP ÷ (FP + TN)

ในตัวอย่างของเรา เมื่อใช้ threshold = 0.5 เรารู้ว่า TPR = **0.75**  
และ FPR = 0 ÷ 2 = **0**

หากเปลี่ยน threshold ที่ใช้ในการทำนายว่าค่าเป็น *true (1)* จำนวนการทำนาย positive และ negative ก็จะเปลี่ยนไป  
ซึ่งส่งผลให้ค่า TPR และ FPR เปลี่ยนตาม

สองเมตริกนี้มักใช้เพื่อวิเคราะห์โมเดลโดยการวาดกราฟ *ROC (Receiver Operating Characteristic)*  
กราฟนี้แสดงความสัมพันธ์ระหว่าง TPR และ FPR ในแต่ละค่าของ threshold ที่อยู่ระหว่าง 0.0 ถึง 1.0

![Diagram of a ROC curve.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/roc-chart.png)

กราฟ ROC สำหรับโมเดลที่สมบูรณ์แบบจะพุ่งตรงขึ้นตามแกน *TPR* ทางซ้าย และขนานไปตามแกน *FPR* ด้านบน  
เนื่องจากพื้นที่ใต้กราฟมีขนาด 1x1 พื้นที่ใต้เส้นโค้งของโมเดลที่สมบูรณ์แบบจะเท่ากับ **1.0**  
แปลว่าโมเดลทำนายถูกต้อง 100%

ในทางตรงข้าม เส้นทแยงจากมุมล่างซ้ายไปยังมุมบนขวา แสดงถึงผลลัพธ์จากการสุ่มเดา  
ซึ่งให้ค่า *AUC* เท่ากับ **0.5** หรือทำนายถูกประมาณ 50% เท่านั้น

สำหรับโมเดลทำนายโรคเบาหวานของเรา กราฟ ROC ที่ได้มีพื้นที่ใต้เส้นโค้ง (AUC) เท่ากับ **0.875**  
เนื่องจากค่า AUC > 0.5 จึงสรุปได้ว่าโมเดลนี้สามารถทำนายว่าใครเป็นเบาหวานได้ *ดีกว่าการสุ่มเดา*

##### สรุป
|**คำ**|**หมายถึง…**|**จำว่า…**|
|---|---|---|
|TPR|จับของจริงได้กี่คน|ไม่พลาดของจริง|
|FPR|ฟ้องคนบริสุทธิ์กี่คน|ฟ้องเกิน|
|ROC|กราฟเปรียบเทียบ TPR/FPR|ดูภาพรวมการทำนาย|
|AUC|พื้นที่ใต้ ROC|คะแนนรวมของโมเดล|
