
ก่อนที่เราจะไปสำรวจความสามารถของ _text analytics_ ในบริการ _Azure AI Language_ เรามาทำความเข้าใจหลักการทั่วไปและเทคนิคที่นิยมใช้ในการวิเคราะห์ข้อความ (_text analysis_) และงานอื่น ๆ ที่เกี่ยวข้องกับ _natural language processing (NLP)_

เทคนิคเริ่มต้นบางอย่างที่ใช้ในการวิเคราะห์ข้อความด้วยคอมพิวเตอร์นั้น อาศัยการวิเคราะห์เชิงสถิติของกลุ่มข้อความ หรือที่เรียกว่า _corpus_ เพื่อหา _ความหมายโดยรวม_ ของข้อความ พูดง่าย ๆ ก็คือ ถ้าคุณสามารถระบุคำที่ถูกใช้บ่อยที่สุดในเอกสารได้ คุณก็มักจะสามารถเข้าใจได้ว่าเอกสารนั้น _เกี่ยวข้องกับเรื่องอะไร_

## Tokenization

ขั้นตอนแรกของการวิเคราะห์ _corpus_ คือการแยกข้อความออกเป็นหน่วยย่อยที่เรียกว่า _tokens_ เพื่อให้ง่ายต่อความเข้าใจ เราอาจมองว่า _token_ คือแต่ละคำที่ไม่ซ้ำกันในข้อความ แต่ในความเป็นจริง _token_ อาจรวมถึงส่วนของคำ หรือการรวมกันของคำและเครื่องหมายวรรคตอนด้วย

ตัวอย่างเช่น พิจารณาวลีจากสุนทรพจน์ของประธานาธิบดีสหรัฐที่ว่า `"we choose to go to the moon"`  

ข้อความนี้สามารถแบ่งออกเป็น _tokens_ ดังนี้ พร้อมหมายเลขระบุแต่ละตัว:

```text
1. we 
2. choose
3. to
4. go
5. the
6. moon
```

สังเกตว่า `"to"` (token หมายเลข 3) ปรากฏอยู่ในข้อความนี้ถึงสองครั้ง  
ดังนั้นวลี `"we choose to go to the moon"` สามารถแทนด้วย _tokens_ ได้เป็นชุด {1,2,3,4,3,5,6}

_หมายเหตุ_  
ในตัวอย่างนี้ เราใช้วิธีง่าย ๆ โดยการสร้าง _token_ สำหรับแต่ละคำที่ไม่ซ้ำกันในข้อความ อย่างไรก็ตาม ในความเป็นจริง การทำ _tokenization_ อาจต้องพิจารณาแนวคิดอื่นเพิ่มเติม ขึ้นอยู่กับลักษณะของปัญหา _NLP_ ที่คุณพยายามจะแก้ เช่น:

- _Text normalization_: ก่อนจะสร้าง _tokens_ เราอาจเลือกที่จะ _normalize_ ข้อความก่อน เช่น การลบเครื่องหมายวรรคตอน และแปลงคำทั้งหมดเป็นตัวพิมพ์เล็ก สำหรับการวิเคราะห์ที่อาศัยแค่ _ความถี่ของคำ_ วิธีนี้ช่วยเพิ่ม _performance_ ได้ดี อย่างไรก็ตาม อาจทำให้ _ความหมาย_ บางอย่างหายไป เช่น ประโยค `"Mr Banks has worked in many banks."`  
  หากเราต้องการแยกแยะระหว่างชื่อบุคคล `"Mr Banks"` กับคำว่า `"banks"` ที่หมายถึงธนาคาร หรือแยก `"banks."` ออกจาก `"banks"` (เนื่องจากจุด "." บอกว่าคำนั้นอยู่ท้ายประโยค) เราอาจไม่ต้องการ normalize มากเกินไป

- _Stop word removal_: _stop words_ คือคำที่มักจะถูกตัดออกจากการวิเคราะห์ เช่น `"the"`, `"a"`, หรือ `"it"` เพราะถึงแม้จะทำให้ประโยคอ่านเข้าใจง่าย แต่ก็ไม่ได้เพิ่ม _ความหมายสำคัญ_ อะไรมากนัก การตัดคำเหล่านี้ออกสามารถช่วยให้ระบบ _text analysis_ เน้นคำที่สำคัญได้ชัดเจนขึ้น

- _n-grams_: คือกลุ่มคำหลายคำ เช่น `"I have"` หรือ `"he walked"` โดยคำเดียวเรียกว่า _unigram_, สองคำเรียกว่า _bi-gram_, สามคำเรียกว่า _tri-gram_ และอื่น ๆ การวิเคราะห์แบบเป็นกลุ่มคำช่วยให้โมเดล _machine learning_ เข้าใจความหมายได้ดีขึ้น

- _Stemming_: เป็นเทคนิคที่ใช้ _algorithm_ เพื่อลดคำให้อยู่ในรูปฐาน เช่น `"power"`, `"powered"`, และ `"powerful"` จะถูกตีความว่าเป็น _token_ เดียวกัน ช่วยลดความซ้ำซ้อนในการวิเคราะห์และจับ _ความหมายหลัก_ ของคำได้ดีขึ้น

## Frequency analysis

หลังจากที่เราทำ _tokenization_ แล้ว เราสามารถวิเคราะห์ต่อโดยการนับจำนวนครั้งที่แต่ละ _token_ ปรากฏขึ้น คำที่ใช้บ่อยที่สุด (ยกเว้น _stop words_ เช่น `"a"`, `"the"` เป็นต้น) มักจะบอกได้ว่า _ข้อความนั้นเกี่ยวข้องกับเรื่องอะไร_ เช่น ถ้าเราวิเคราะห์สุนทรพจน์ `"go to the moon"` ที่ยกตัวอย่างไปก่อนหน้า คำที่พบบ่อยที่สุดคือ `"new"`, `"go"`, `"space"`, และ `"moon"`  
ถ้าเราใช้การ _tokenization_ แบบ _bi-grams_ (จับคำเป็นคู่) จะพบว่า _bi-gram_ ที่พบบ่อยที่สุดคือ `"the moon"`  
จากข้อมูลนี้ เราสามารถเดาได้ไม่ยากว่าเนื้อหาหลักของข้อความนี้คือเรื่อง _การเดินทางในอวกาศ_ และ _การไปดวงจันทร์_

_เคล็ดลับ_  
การวิเคราะห์แบบง่ายโดยการนับ _ความถี่ของ token_ เหมาะกับการวิเคราะห์เอกสารฉบับเดียว แต่ถ้าคุณต้องการแยกความแตกต่างระหว่างหลายเอกสารภายใน _corpus_ เดียวกัน คุณจะต้องใช้วิธีที่สามารถระบุได้ว่า _token ใดสำคัญกับแต่ละเอกสาร_ มากที่สุด

เทคนิคหนึ่งที่นิยมใช้คือ _term frequency - inverse document frequency (TF-IDF)_ ซึ่งจะคำนวณคะแนนตามความถี่ของคำในเอกสารหนึ่ง เทียบกับความถี่ของคำนั้นในเอกสารทั้งหมดในชุด  
วิธีนี้จะช่วยเน้นคำที่ _มีความสำคัญเฉพาะในเอกสารนั้น_ แต่ _ไม่ปรากฏบ่อยในเอกสารอื่น_ ทำให้สามารถระบุคำที่ _สื่อถึงเนื้อหาเฉพาะ_ ได้แม่นยำยิ่งขึ้น

## Machine learning for text classification

อีกหนึ่งเทคนิคที่มีประโยชน์ในการวิเคราะห์ข้อความคือการใช้ _classification algorithm_ เช่น _logistic regression_ เพื่อฝึก _machine learning model_ ให้สามารถ _จำแนกข้อความ_ ตามหมวดหมู่ที่เรารู้ล่วงหน้าแล้ว

ตัวอย่างที่พบบ่อยของเทคนิคนี้คือการฝึกโมเดลให้สามารถ _จำแนกข้อความว่าเป็นบวกหรือเป็นลบ_ เพื่อนำไปใช้ในงาน _sentiment analysis_ หรือ _opinion mining_

ตัวอย่างเช่น พิจารณารีวิวร้านอาหารต่อไปนี้ ซึ่งได้ถูกระบุฉลากไว้แล้วว่าเป็น **0** (_negative_) หรือ **1** (_positive_):

```
- *The food and service were both great*: 1
- *A really terrible experience*: 0
- *Mmm! tasty food and a fun vibe*: 1
- *Slow service and substandard food*: 0
```

เมื่อคุณมีรีวิวที่มีการ _label_ ไว้เพียงพอ คุณสามารถฝึก _classification model_ โดยใช้ข้อความที่ผ่านการ _tokenize_ แล้วเป็น _features_ และใช้ค่าความรู้สึก (0 หรือ 1) เป็น _label_ โมเดลจะสามารถเรียนรู้ _ความสัมพันธ์ระหว่าง token กับ sentiment_ ได้ เช่น ถ้ารีวิวมีคำอย่าง `"great"`, `"tasty"` หรือ `"fun"` ก็มีแนวโน้มที่จะถูกจำแนกว่าเป็น **1** (_positive_) ในทางกลับกัน ถ้ามีคำอย่าง `"terrible"`, `"slow"` หรือ `"substandard"` ก็มีแนวโน้มที่จะถูกจำแนกว่าเป็น **0** (_negative_)

## Semantic language models

เมื่อเทคโนโลยี _NLP_ พัฒนาไปมากขึ้น ความสามารถในการฝึกโมเดลที่เข้าใจ _ความสัมพันธ์เชิงความหมาย_ ระหว่าง _tokens_ ก็ทำให้เกิด _language models_ ที่ทรงพลังมากขึ้น หัวใจสำคัญของโมเดลเหล่านี้คือการแปลง _tokens_ ให้อยู่ในรูปของเวกเตอร์ (อาร์เรย์ของตัวเลขหลายค่า) ซึ่งเราเรียกว่า _embeddings_

เราสามารถนึกภาพ _embedding vector_ ว่าเป็นเหมือน _พิกัดในพื้นที่หลายมิติ_ โดยที่แต่ละ _token_ จะอยู่ในตำแหน่งใดตำแหน่งหนึ่ง และถ้า _token_ สองตัวอยู่ใกล้กันในมิติบางอย่าง ก็แปลว่ามี _ความหมายที่ใกล้เคียงกัน_ หรือเกี่ยวข้องกันนั่นเอง กล่าวคือ คำที่เกี่ยวข้องกันจะถูกจัดให้อยู่ใกล้กันใน _vector space_

ยกตัวอย่างง่าย ๆ สมมุติว่าเราใช้เวกเตอร์ที่มี 3 ค่า (_สามมิติ_) ในการแทน _embeddings_ ของแต่ละ token เช่น:

```
- 4 ("dog"): [10.3.2]
- 5 ("bark"): [10,2,2]
- 8 ("cat"): [10,3,1]
- 9 ("meow"): [10,2,1]
- 10 ("skateboard"): [3,3,1]
```

เราสามารถ _พล็อตตำแหน่งของ tokens_ จากเวกเตอร์เหล่านี้ลงใน _พื้นที่สามมิติ_ ได้ ตัวอย่างเช่น:

![A diagram of tokens plotted on a three-dimensional space.](https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-text-with-text-analytics-service/media/example-embeddings-graph.png)

ตำแหน่งของ _tokens_ ใน _embeddings space_ จะมีข้อมูลเกี่ยวกับความใกล้ชิดเชิงความหมายระหว่างกันอยู่ด้วย  
เช่น _token_ ของคำว่า `"dog"` จะอยู่ใกล้กับ `"cat"` และยังใกล้กับ `"bark"` ด้วย ในขณะเดียวกัน `"cat"` กับ `"bark"` ก็จะอยู่ใกล้กับ `"meow"`  แต่คำว่า `"skateboard"` จะอยู่ห่างจากคำเหล่านี้ เพราะมี _semantic meaning_ ที่แตกต่างกันโดยสิ้นเชิง

โมเดลภาษาที่ใช้ในอุตสาหกรรมทุกวันนี้ยังคงยึดหลักการเดียวกันนี้ เพียงแต่มี _ความซับซ้อนมากขึ้น_ เช่น เวกเตอร์ที่ใช้มักจะมี _จำนวนมิติมากกว่าเดิมมาก_ และมีหลายวิธีในการคำนวณ _embeddings_ สำหรับชุดของ _tokens_ ซึ่งแต่ละวิธีก็จะส่งผลให้โมเดล _NLP_ ทำนายผลต่างกันได้

โดยภาพรวม แนวทางของระบบ _natural language processing_ สมัยใหม่มักมีโครงสร้างดังนี้: เรานำ _corpus_ ข้อความจำนวนมากมาทำ _tokenization_ แล้วใช้ในการฝึก _language models_  ซึ่งโมเดลเหล่านี้สามารถนำไปใช้กับ _งาน NLP_ ได้หลากหลายประเภท

![A diagram of the process to tokenize text and train a language model that supports natural language processing tasks.](https://learn.microsoft.com/en-us/training/wwl-data-ai/analyze-text-with-text-analytics-service/media/language-model.png)

งาน _NLP_ ที่พบบ่อยและสามารถทำได้โดยใช้ _language models_ ได้แก่:

- การวิเคราะห์ข้อความ (_text analysis_) เช่น การดึงคำสำคัญ หรือการระบุชื่อบุคคล/สถานที่/องค์กร (_named entities_) จากข้อความ  
- การวิเคราะห์ _sentiment_ และ _opinion mining_ เพื่อจำแนกข้อความว่าเป็น _positive_ หรือ _negative_  
- การแปลภาษา (_machine translation_) โดยแปลข้อความจากภาษาหนึ่งไปอีกภาษาโดยอัตโนมัติ  
- การสรุปข้อความ (_summarization_) โดยย่อใจความสำคัญจากข้อความจำนวนมาก  
- การสร้างระบบสนทนา (_conversational AI_) เช่น _bots_ หรือ _digital assistants_ ที่สามารถเข้าใจภาษาธรรมชาติและตอบกลับอย่างเหมาะสม

ความสามารถเหล่านี้ (และอื่น ๆ อีกมาก) มีอยู่ใน _Azure AI Language_ ซึ่งเราจะไปสำรวจกันต่อในขั้นถัดไป