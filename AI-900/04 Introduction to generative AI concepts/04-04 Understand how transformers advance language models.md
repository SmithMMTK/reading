[[reading/AI-900/02 Machine learning concepts/02-07 ml_concept Transformers]]

แอปพลิเคชัน _generative AI_ ที่เราใช้กันในปัจจุบันเป็นไปได้เพราะมีการนำ _Transformer architecture_ มาใช้งาน

สถาปัตยกรรม Transformer ถูกนำเสนอครั้งแรกในงานวิจัยชื่อ _Attention is all you need_ โดย Vaswani และคณะ ในปี 2017

การมาของ _Transformer_ ได้เปลี่ยนแปลงความสามารถของโมเดลในการ _เข้าใจและสร้างข้อความ_ ไปอย่างมาก โดยเพิ่มความแม่นยำและประสิทธิภาพในการจัดการกับบริบทของคำ

ตั้งแต่นั้นมา ได้มีการพัฒนาโมเดลต่าง ๆ ที่อิงกับ _Transformer architecture_ และมีการปรับแต่งเพื่อให้เหมาะสมกับงาน _NLP_ แบบเฉพาะด้าน

## Understand Transformer architecture

ใน _Transformer architecture_ ดั้งเดิมประกอบด้วย 2 ส่วนหลักคือ:

- _Encoder_: ทำหน้าที่ประมวลผลลำดับข้อความที่ป้อนเข้า และสร้าง _representation_ ที่เก็บบริบทของแต่ละ _token_
- _Decoder_: ทำหน้าที่สร้างลำดับคำตอบ โดยอ้างอิงจาก _representation_ ของ encoder และทำนาย token ถัดไปในลำดับ

นวัตกรรมที่สำคัญที่สุดของ _Transformer_ คือ:

- _Positional encoding_: ทำให้โมเดลรู้ตำแหน่งของคำในประโยค แม้ว่าจะไม่มีโครงสร้างลำดับแบบใน RNN
- _Multi-head attention_: ทำให้โมเดลสามารถ _โฟกัสกับหลายบริบทพร้อมกัน_ และเรียนรู้ความสัมพันธ์ระหว่างคำในหลายมุมมอง

ภาพรวมแบบย่อของสถาปัตยกรรมนี้จะแสดงให้เห็นการทำงานร่วมกันระหว่าง encoder และ decoder โดยมี attention เชื่อมโยงข้อมูลระหว่างกัน

![A diagram of the Transformer architcture with the encoding and decoding layers.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-generative-ai/media/simplified-transformer-architecture.png)

- ใน _encoder layer_ ลำดับข้อความที่ป้อนเข้าจะถูกเข้ารหัสด้วย _positional encoding_ ก่อน  จากนั้นใช้ _multi-head attention_ เพื่อสร้าง _representation_ ที่เข้าใจบริบทของข้อความทั้งประโยค

- ใน _decoder layer_ ลำดับผลลัพธ์ (ที่ยังไม่สมบูรณ์) จะถูกเข้ารหัสในลักษณะคล้ายกัน โดยเริ่มจาก _positional encoding_ แล้วตามด้วย _multi-head attention_ หลังจากนั้น _multi-head attention_ จะถูกใช้ซ้ำอีกครั้งภายใน decoder เพื่อนำเอาผลลัพธ์จาก _encoder_ มารวมกับ _representation_ ของลำดับ output ที่ป้อนเข้ามาใน decoder 
  
  ผลลัพธ์จากขั้นตอนทั้งหมดนี้ ทำให้โมเดลสามารถ _สร้างข้อความที่ตอบกลับได้อย่างมีบริบท_ และต่อเนื่องกับ input ที่ได้รับ

## Understand positional encoding

ตำแหน่งของคำ และลำดับของคำในประโยค มีความสำคัญอย่างมากต่อการเข้าใจความหมายของข้อความ  
เพื่อรวมข้อมูลเหล่านี้เข้ากับโมเดล _โดยไม่ต้องประมวลผลทีละคำ_ แบบ RNN, _Transformer_ จึงใช้แนวคิดที่เรียกว่า _positional encoding_

ก่อนมี Transformer, โมเดลภาษาจะใช้ _word embeddings_ เพียงอย่างเดียวในการแปลงคำให้เป็นเวกเตอร์  
แต่ในสถาปัตยกรรมของ Transformer จะใช้ _positional encoding_ ซึ่งเป็นการ _รวมกันของ word embeddings และ positional vectors_

ด้วยวิธีนี้ เวกเตอร์ที่ได้จะมีทั้ง _ความหมายของคำ_ และ _ตำแหน่งของคำในประโยค_

เพื่อเข้ารหัสตำแหน่งของคำในประโยค เราอาจใช้ _เลขลำดับ (index)_ เช่น:

- คำแรกในประโยค = ตำแหน่ง 0  
- คำที่สอง = ตำแหน่ง 1  
- คำที่สาม = ตำแหน่ง 2  
...และต่อไปเรื่อย ๆ

ตำแหน่งเหล่านี้จะถูกแปลงเป็นเวกเตอร์ แล้วนำไปบวกกับ _word embeddings_ เพื่อให้โมเดลเรียนรู้ได้ว่าคำนั้น ๆ อยู่ในตำแหน่งไหนของประโยค

| Token       | Index value |
| ----------- | ----------- |
| The         | 0           |
| work        | 1           |
| of          | 2           |
| William     | 3           |
| Shakespeare | 4           |
| inspired    | 5           |
| many        | 6           |
| movies      | 7           |
| ...         | ...         |
ยิ่งข้อความหรือ _sequence_ ยาวเท่าไร ค่าลำดับตำแหน่ง (_index_) ก็จะยิ่งมีค่ามากขึ้น  
แม้ว่าการใช้ _ค่าตำแหน่งที่ไม่ซ้ำกัน_ สำหรับแต่ละคำจะเป็นวิธีที่เรียบง่าย แต่ค่าตัวเลขเหล่านี้ _ไม่มีความหมายเชิงบริบท_ และเมื่อค่ามากขึ้นเรื่อย ๆ ก็อาจทำให้โมเดล _ไม่เสถียรระหว่างการฝึก_

กล่าวคือ ค่าตำแหน่งแบบเลขตรง ๆ อาจส่งผลเสียต่อการเรียนรู้ของโมเดล เพราะไม่มีการแสดงความสัมพันธ์ระหว่างตำแหน่ง เช่น ตำแหน่ง 2 ใกล้กับ 3 แต่ห่างจาก 10 มาก เป็นต้น

ดังนั้น _Transformer_ จึงใช้ _positional vectors_ ที่มีลักษณะเป็นเวกเตอร์แบบพิเศษ ซึ่งสร้างจากฟังก์ชันเชิงคณิตศาสตร์ เช่น sine และ cosine เพื่อให้โมเดลสามารถเรียนรู้ _ความสัมพันธ์เชิงตำแหน่ง_ ได้อย่างราบรื่นและมีเสถียรภาพมากขึ้น

## Understand attention

เทคนิคที่สำคัญที่สุดของ _Transformer_ ในการประมวลผลข้อความคือการใช้ _attention_ แทน _recurrence_  
ซึ่งทำให้ Transformer เป็นทางเลือกที่มีประสิทธิภาพกว่า _RNNs_

_RNNs_ ต้องประมวลผลคำทีละคำแบบลำดับ ซึ่งใช้ทรัพยากรสูง ในขณะที่ _Transformer_ สามารถประมวลผลคำ _พร้อมกันทั้งหมด_ แบบขนาน โดยใช้กลไก _attention_

_Attention_ (หรือที่เรียกว่า _self-attention_ หรือ _intra-attention_) คือกลไกที่ช่วยให้โมเดล _เชื่อมโยงข้อมูลใหม่กับข้อมูลที่เคยเรียนรู้มา_ เพื่อทำความเข้าใจว่าข้อมูลใหม่มีความหมายอย่างไร

Transformer ใช้ _attention function_ ที่ประกอบด้วย:

- _Query_: เวกเตอร์ที่แทนคำใหม่ที่กำลังจะถูกประมวลผล
- _Key_: เวกเตอร์ที่แทนคำในชุดข้อมูลที่เรียนรู้ไว้
- _Value_: ค่าความหมายที่เชื่อมโยงกับ key นั้น

ลองดูตัวอย่างง่าย ๆ:  
สมมุติเรากำลัง encode ประโยค "Vincent van Gogh is a painter, known for his stunning and emotionally expressive artworks."

เมื่อกำหนดคำว่า _"Vincent van Gogh"_ เป็น _query_  
ระบบอาจแมป query นี้กับ _key_ ที่เป็น "Vincent van Gogh" และมี _value_ คือ "painter"

ระบบจะเก็บ _keys_ และ _values_ เหล่านี้ไว้ในตาราง เพื่อสามารถใช้อ้างอิงเมื่อทำ _decoding_ ในขั้นตอนต่อไปได้

|Keys|Values|
|---|---|
|Vincent Van Gogh|Painter|
|William Shakespeare|Playwright|
|Charles Dickens|Writer|
เมื่อมีประโยคใหม่เช่น  
"Shakespeare's work has influenced many movies, mostly thanks to his work as a ...."  
โมเดลสามารถเติมคำที่หายไปได้ โดยใช้คำว่า _Shakespeare_ เป็น _query_

โมเดลจะค้นหา _query_ นี้ในตาราง _keys_ และ _values_ ซึ่งจะพบว่า "Shakespeare" ใกล้เคียงกับ key ที่เป็น "William Shakespeare"  
จึงเลือก _value_ ที่เชื่อมโยงกัน คือคำว่า _playwright_ เป็นคำตอบที่เหมาะสม

การคำนวณ _attention function_ จะเริ่มจากการแปลง _query_, _keys_, และ _values_ ทั้งหมดให้เป็น _เวกเตอร์_

จากนั้นจะคำนวณ _scaled dot-product_ ระหว่าง _query vector_ กับแต่ละ _key vector_  
_dot-product_ ใช้ดู _มุม_ ระหว่างเวกเตอร์ — ถ้ามุมใกล้กัน (ทิศทางคล้ายกัน) ผลลัพธ์จะมีค่ามาก

ภายใน attention function จะใช้ฟังก์ชัน _softmax_ กับค่าที่ได้จาก scaled dot-product เพื่อสร้าง _distribution_ ของความน่าจะเป็น  
กล่าวคือ softmax จะบอกว่า _keys ตัวไหนใกล้กับ query มากที่สุด_

โมเดลจะเลือก key ที่มีความน่าจะเป็นสูงสุด แล้วนำ _value ที่เกี่ยวข้อง_ มาเป็น _output_ ของ attention

สถาปัตยกรรม _Transformer_ ยังใช้เทคนิคที่เรียกว่า _multi-head attention_ หมายถึงคำในประโยคจะถูกประมวลผลผ่าน attention หลายรอบพร้อมกัน  
แต่ละรอบอาจมองคำจาก _บริบทที่ต่างกัน_ ช่วยให้โมเดลเข้าใจ _หลายแง่มุม_ ของคำหรือประโยคได้ดีขึ้น

Transformer ทำให้เราสามารถฝึกโมเดลได้อย่างมีประสิทธิภาพมากขึ้น เพราะไม่ต้องประมวลผลคำทีละตัวแบบลำดับ  
_attention_ ช่วยให้โมเดลประมวลผล token หลายตัว _พร้อมกัน_ และจากมุมมองที่หลากหลาย

ต่อไป เราจะเรียนรู้ว่า _language models_ ประเภทต่าง ๆ ที่ใช้กับ _generative AI_ มีอะไรบ้าง