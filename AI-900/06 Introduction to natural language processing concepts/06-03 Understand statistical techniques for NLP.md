
เทคนิคทางสถิติที่สำคัญสองอย่างซึ่งเป็นรากฐานของการประมวลผลภาษาธรรมชาติ (_natural language processing_ หรือ _NLP_) ได้แก่ _Naïve Bayes_ และ _Term Frequency - Inverse Document Frequency (TF-IDF)_

## Understanding Naïve Bayes

_Naïve Bayes_ เป็นเทคนิคทางสถิติที่ถูกนำมาใช้ครั้งแรกในการกรองอีเมล โดยใช้เพื่อแยกความแตกต่างระหว่างอีเมลประเภท _spam_ กับ _not spam_ วิธีการคือเปรียบเทียบข้อความในสองเอกสาร แล้วหาว่าคำ (_tokens_) ใดที่มักจะปรากฏในอีเมลที่ถูกระบุว่าเป็น _spam_

_Naïve Bayes classifiers_ จะช่วยหาว่าคำกลุ่มไหนที่มักเจอใน _spam_ เท่านั้น และไม่ค่อยเจอในอีเมลทั่วไป กลุ่มคำเหล่านี้เรียกว่า _bag-of-words features_ เช่นคำว่า _miracle cure_, _lose weight fast_ หรือ _anti-aging_ มักพบในอีเมลหลอกขายผลิตภัณฑ์สุขภาพ มากกว่าที่จะพบในอีเมลทั่วไป

แม้ว่า _Naïve Bayes_ จะมี _accuracy_ ที่ดีกว่าวิธีที่อิงตามกฎแบบง่าย (_rule-based models_) ในงาน _text classification_ แต่มันยังถือว่าเป็นวิธีพื้นฐาน เพราะจะพิจารณาแค่การมีอยู่ของคำเท่านั้น โดยไม่สนใจตำแหน่งของคำนั้นในข้อความ

## Understanding TF-IDF

_Term Frequency - Inverse Document Frequency (TF-IDF)_ เป็นเทคนิคที่ใช้หลักการคล้ายกับการนับคำ โดยจะเปรียบเทียบความถี่ของคำหนึ่งในเอกสารเดียว กับความถี่ของคำนั้นใน _corpus_ หรือเอกสารทั้งหมดที่มีอยู่

เทคนิคนี้ช่วยให้เราเข้าใจว่าแต่ละคำมีความสำคัญมากน้อยแค่ไหนในบริบทของเอกสารนั้น ๆ เช่น ถ้าคำ ๆ หนึ่งเจอบ่อยในเอกสารเฉพาะเรื่อง แต่เจอน้อยมากในเอกสารอื่น ๆ ก็แสดงว่าคำนี้น่าจะเกี่ยวข้องกับหัวข้อเฉพาะนั้น

TF-IDF จึงมักใช้ในงาน _information retrieval_ หรือการค้นหาข้อมูล เพื่อช่วยให้ระบบรู้ว่าจะเน้นคำไหนเป็นพิเศษตอนค้นหาเทคนิคนี้ช่วยให้ระบบจัดประเภทเอกสารตามหัวข้อได้แม่นยำยิ่งขึ้น โดยดูจากบริบทการใช้คำ ไม่ใช่แค่จำนวนครั้งที่คำปรากฏเท่านั้น

> ในบริบทของ _natural language processing (NLP)_ คำว่า _corpus_ หมายถึงชุดเอกสารข้อความขนาดใหญ่ที่มีโครงสร้างชัดเจน ซึ่งถูกนำมาใช้สำหรับงาน _machine learning_ 
> 
> _corpora_ (พหูพจน์ของ corpus) เป็นทรัพยากรที่สำคัญมาก เพราะใช้ในการฝึก (_training_) ทดสอบ (_testing_) และประเมินผล (_evaluating_) โมเดลต่าง ๆ ในงาน _NLP_ เช่น การแปลภาษา การวิเคราะห์ความรู้สึก หรือการจำแนกประเภทเอกสาร
> 
> การมี _corpus_ ที่ดีจะช่วยให้โมเดลเข้าใจรูปแบบภาษาได้แม่นยำมากขึ้น

ตัวอย่างเช่น หลังจากที่เรา _tokenize_ คำในประโยค “we choose to go to the moon” แล้ว เราสามารถวิเคราะห์จำนวนครั้งที่แต่ละ _token_ ปรากฏขึ้นได้ คำที่ใช้บ่อย (ยกเว้น _stop words_ เช่น “a”, “the” เป็นต้น) มักช่วยให้เราพอเดาได้ว่าข้อความนั้นเกี่ยวกับอะไร ในสุนทรพจน์ “go to the moon” คำที่พบบ่อยคือ _“new”_, _“go”_, _“space”_, และ _“moon”_ ซึ่งเป็นคำที่บ่งบอกว่าข้อความนี้น่าจะเกี่ยวกับการเดินทางใน _อวกาศ_ หรือ _space travel_

ถ้าเรา _tokenize_ เป็น _bi-grams_ หรือคู่คำ เช่น “go to”, “the moon” เราจะพบว่า _“the moon”_ เป็น _bi-gram_ ที่พบมากที่สุด ซึ่งช่วยยืนยันว่าเนื้อหาหลักเกี่ยวข้องกับดวงจันทร์ การวิเคราะห์แบบง่ายโดยนับจำนวนครั้งที่แต่ละ _token_ ปรากฏ (_simple frequency analysis_) สามารถใช้ได้ผลดีในเอกสารเดียว แต่ถ้าเราต้องการเปรียบเทียบความสำคัญของคำระหว่างหลายเอกสารใน _corpus_ เดียวกัน เราต้องมีวิธีที่ช่วยระบุว่าคำไหนเกี่ยวข้องกับเอกสารใดมากที่สุด

_TF-IDF_ ช่วยแก้ปัญหานี้โดยการคำนวณคะแนนตามความถี่ของคำในเอกสารหนึ่ง เทียบกับความถี่ของคำนั้นใน _corpus_ ทั้งหมด คำที่เจอบ่อยในเอกสารใดเอกสารหนึ่ง แต่เจอน้อยในเอกสารอื่น ๆ จะได้คะแนนสูง ซึ่งแปลว่าคำนั้นมี _ความเกี่ยวข้อง_ (_relevance_) สูงกับเอกสารนั้น 

ต่อไปเราจะไปดูเทคนิค _deep learning_ ที่ใช้สร้างโมเดลความเข้าใจความหมาย (_semantic models_) ในยุคปัจจุบันกัน

