
หนึ่งในเทคนิคที่ใช้วิเคราะห์ข้อความด้วยคอมพิวเตอร์ในยุคแรก ๆ คือการวิเคราะห์เชิงสถิติของข้อความจำนวนมาก (_corpus_) เพื่อพยายามหาความหมายเชิงเนื้อหา

พูดง่าย ๆ คือ ถ้าเราสามารถหาคำที่ถูกใช้บ่อยที่สุดในเอกสารใดเอกสารหนึ่งได้ เราก็มักจะสามารถเข้าใจได้ว่าเอกสารนั้นพูดถึงเรื่องอะไร

## Tokenization

ขั้นตอนแรกของการวิเคราะห์ _corpus_ คือการแยกข้อความออกเป็น _tokens_

เพื่อให้ง่ายต่อความเข้าใจ ลองนึกว่า _token_ แต่ละตัวคือคำแต่ละคำในข้อความฝึกฝน ถึงแม้ในความเป็นจริง _tokens_ อาจเป็นแค่ส่วนหนึ่งของคำ หรือรวมกับเครื่องหมายวรรคตอนก็ได้

ตัวอย่างเช่น วลีจากสุนทรพจน์ประธานาธิบดีของสหรัฐอเมริกา: "we choose to go to the moon"  
วลีนี้สามารถแยกออกเป็น _tokens_ ได้ดังนี้ พร้อมกับหมายเลขประจำแต่ละ _token_:

```text
1. we 
2. choose
3. to
4. go
5. the
6. moon
```

สังเกตว่า "to" (_token_ หมายเลข 3) ปรากฏขึ้นสองครั้งใน _corpus_ นี้  
ดังนั้นวลี "we choose to go to the moon" สามารถแทนด้วยลำดับของ _tokens_ ได้เป็น {1,2,3,4,3,5,6}

แม้ในตัวอย่างนี้เราจะใช้วิธีง่าย ๆ โดยกำหนด _token_ ให้แต่ละคำที่ไม่ซ้ำกันในข้อความ  
แต่ในการทำ _tokenization_ จริง ๆ อาจมีแนวคิดเพิ่มเติมที่ใช้ขึ้นอยู่กับลักษณะของปัญหา _NLP_ ที่ต้องการแก้ เช่น:

| **Concept**            | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                |
| ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Text normalization** | ก่อนจะสร้าง _tokens_ เราอาจต้อง _normalize_ ข้อความก่อน เช่น ลบเครื่องหมายวรรคตอน และเปลี่ยนเป็นตัวพิมพ์เล็กทั้งหมด วิธีนี้ช่วยให้การวิเคราะห์ที่เน้น _word frequency_ มีประสิทธิภาพมากขึ้น  <br>แต่ก็อาจทำให้สูญเสียความหมายบางอย่าง เช่น "Mr Banks has worked in many banks." อาจต้องการแยกความหมายของ "Mr Banks" (ชื่อคน) กับ "banks" (ธนาคาร) รวมถึงแยก "banks." กับ "banks" เพราะเครื่องหมายจุดสื่อว่าคำนี้อยู่ท้ายประโยค |
| **Stop word removal**  | คือการตัดคำที่ไม่สื่อความหมายสำคัญออกจากการวิเคราะห์ เช่น "the", "a", หรือ "it" เพื่อช่วยให้ระบบเน้นไปที่คำสำคัญแทน                                                                                                                                                                                                                                                                                                            |
| **n-grams**            | การจับกลุ่มคำที่อยู่ติดกัน เช่น "I have" หรือ "he walked" ซึ่ง "I" กับ "have" แต่ละคำเรียกว่า _unigram_ ถ้าเอา 2 คำรวมกันเป็น _bi-gram_ และ 3 คำเป็น _tri-gram_  <br>การใช้ _n-grams_ ช่วยให้โมเดลเข้าใจความหมายของกลุ่มคำมากขึ้น<br><br>[[reading/AI-900/06 Introduction to natural language processing concepts/n-gram]]                                                                                                                                                                            |
| **Stemming**           | เป็นเทคนิคที่ทำให้คำที่มีรากเดียวกันถูกนับรวมกัน เช่น "power", "powered", และ "powerful" จะถูกตีความว่าเป็น _token_ เดียวกันคือ "power"as being the same token.                                                                                                                                                                                                                                                                |


