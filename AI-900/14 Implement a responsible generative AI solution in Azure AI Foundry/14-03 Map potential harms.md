
ขั้นตอนแรกของกระบวนการ _generative AI อย่างรับผิดชอบ_ คือการ _map_ หรือการระบุ _ความเสี่ยงหรืออันตราย_ ที่อาจส่งผลกระทบต่อโซลูชันที่คุณวางแผนจะสร้าง โดยในขั้นตอนนี้ประกอบด้วย 4 ขั้นตอนย่อย ดังนี้:

![Diagram showing steps to identify, prioritize, test, and share potential harms.](https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-ai-studio/media/identify-harms.png)

1. _Identify potential harms_  ระบุ _อันตรายหรือความเสี่ยงที่อาจเกิดขึ้น_ จากการใช้งานโซลูชันของคุณ เช่น ความเข้าใจผิด การสร้างข้อมูลที่ไม่เหมาะสม หรือการเลือกปฏิบัติ
2. _Prioritize identified harms_ จัดลำดับความสำคัญของอันตรายที่ระบุไว้ โดยพิจารณาจาก _ความรุนแรง_ และ _ความเป็นไปได้ที่จะเกิดขึ้น_
3. _Test and verify the prioritized harms_ ทดสอบและตรวจสอบว่าอันตรายที่มีความสำคัญนั้น _สามารถเกิดขึ้นจริง_ ได้จากผลลัพธ์ที่โมเดลสร้างหรือไม่
4. _Document and share the verified harms_ จัดทำเอกสารและ _แบ่งปัน_ ความเสี่ยงที่ได้รับการยืนยันแล้ว เพื่อให้ทีมและผู้มีส่วนเกี่ยวข้องรับทราบ และใช้เป็นข้อมูลในการวางแผนระยะต่อไป

## 1: Identify potential harms

อันตรายที่อาจเกิดขึ้น (_potential harms_) จากโซลูชัน _generative AI_ ของคุณขึ้นอยู่กับหลายปัจจัย เช่น บริการหรือโมเดลที่ใช้ในการสร้าง _output_ รวมถึงข้อมูลที่นำมาใช้ในการ _fine-tune_ หรือ _ground_ ผลลัพธ์ให้สอดคล้องกับบริบทเฉพาะ

ตัวอย่างความเสี่ยงที่พบบ่อย ได้แก่

- การสร้างเนื้อหาที่ _ไม่เหมาะสม_ มีลักษณะ _เหยียดหยาม_ หรือ _เลือกปฏิบัติ_
- การสร้างเนื้อหาที่มี _ข้อผิดพลาดทางข้อเท็จจริง_ (_factual inaccuracies_)
- การสร้างเนื้อหาที่ _สนับสนุนหรือส่งเสริม_ พฤติกรรมที่ _ผิดกฎหมาย_ หรือ _ผิดจริยธรรม_

เพื่อเข้าใจข้อจำกัดและพฤติกรรมของบริการหรือโมเดลที่ใช้ ควรศึกษาจากเอกสารที่มีให้ เช่น

- [_transparency note_ ของ Azure OpenAI Service](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note) ช่วยให้เข้าใจข้อพิจารณาเฉพาะของบริการและโมเดลที่เกี่ยวข้อง  
- [_OpenAI system card สำหรับโมเดล GPT-4_](https://cdn.openai.com/papers/gpt-4-system-card.pdf)

แนะนำให้ศึกษาแนวทางใน  
- [_Microsoft Responsible AI Impact Assessment Guide_](https://msblogs.thesourcemediaassets.com/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf)  
พร้อมใช้ [_Responsible AI Impact Assessment template_](https://msblogs.thesourcemediaassets.com/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf) เพื่อ _บันทึกและประเมินความเสี่ยง_

สุดท้าย อย่าลืมตรวจสอบ [_ข้อมูลและแนวทางการใช้งาน AI อย่างรับผิดชอบ_](https://learn.microsoft.com/en-us/azure/ai-services/responsible-use-of-ai-overview) ที่เกี่ยวข้องกับทรัพยากรที่คุณใช้ เพื่อช่วยให้คุณสามารถ _ระบุความเสี่ยงที่เกี่ยวข้อง_ ได้อย่างรอบด้าน

## 2: Prioritize the harms

สำหรับ _แต่ละความเสี่ยง_ (_potential harm_) ที่คุณระบุไว้ ให้ประเมินทั้ง _ความเป็นไปได้_ ที่จะเกิดขึ้น และ _ระดับผลกระทบ_ หากมันเกิดขึ้นจริง จากนั้นนำข้อมูลนี้มาใช้ในการจัดลำดับความสำคัญ โดยเริ่มจากความเสี่ยงที่ _มีโอกาสเกิดขึ้นสูง_ และ _ส่งผลกระทบรุนแรง_ เป็นอันดับแรก

การจัดลำดับความสำคัญนี้ควรพิจารณาทั้ง _วัตถุประสงค์หลัก_ ของการใช้โซลูชัน รวมถึง _ความเป็นไปได้ที่จะถูกนำไปใช้ในทางที่ผิด_ ทั้งนี้ การประเมินอาจมีความเป็น _อัตวิสัย_ (_subjective_) ต้องอาศัยการอภิปรายในทีมพัฒนา และอาจต้องปรึกษาผู้เชี่ยวชาญด้านนโยบายหรือกฎหมายด้วย

ตัวอย่างเช่น:  
สมมุติว่าคุณกำลังพัฒนา _smart kitchen copilot_ ที่ให้คำแนะนำเกี่ยวกับสูตรอาหารสำหรับพ่อครัวและผู้ทำอาหารทั่วไป ความเสี่ยงที่อาจพบ ได้แก่

- ระบบให้เวลาปรุงอาหารที่ _ไม่ถูกต้อง_ ทำให้อาหารไม่สุกและอาจก่อให้เกิดโรค  
- ระบบให้สูตร _พิษร้ายแรง_ ที่สามารถสร้างได้จากวัตถุดิบในบ้าน เมื่อผู้ใช้ถาม

แม้ทั้งสองเหตุการณ์จะ _ไม่พึงประสงค์_ แต่คุณอาจพิจารณาว่า การสร้างสูตรพิษมี _ผลกระทบสูงกว่า_ การให้อาหารไม่สุก อย่างไรก็ตาม ในบริบทของการใช้งานจริง อาจพบว่า การให้เวลาปรุงไม่ถูกต้องเกิดขึ้น _บ่อยกว่า_ การมีผู้ใช้ถามหาสูตรพิษโดยตรง

ดังนั้น การตัดสินใจขั้นสุดท้ายเกี่ยวกับลำดับความสำคัญ ควรเป็นผลมาจากการอภิปรายร่วมกันในทีมพัฒนา และอาจรวมถึงการปรึกษาผู้เชี่ยวชาญ เพื่อให้การจัดลำดับ _สะท้อนความเสี่ยงอย่างรอบด้าน_

## 3: Test and verify the presence of harms

เมื่อคุณมีรายการความเสี่ยงที่ถูกจัดลำดับเรียบร้อยแล้ว ขั้นตอนถัดไปคือ _การทดสอบโซลูชัน_ เพื่อ _ตรวจสอบว่าแต่ละความเสี่ยงสามารถเกิดขึ้นได้จริงหรือไม่_ และภายใต้เงื่อนไขแบบใด การทดสอบอาจเผยให้เห็น _ความเสี่ยงใหม่_ ที่ไม่เคยระบุไว้ ซึ่งคุณสามารถนำมาเพิ่มในรายการได้

วิธีที่นิยมใช้ในการทดสอบหาช่องโหว่หรือ _potential harms_ คือการใช้แนวทางที่เรียกว่า _"red team"_ ซึ่งหมายถึงการให้ทีมผู้ทดสอบ _พยายามเจาะระบบ_ เพื่อค้นหาจุดอ่อนและสร้างผลลัพธ์ที่เป็นอันตรายโดยตั้งใจ

ตัวอย่างของการ _red teaming_ สำหรับโซลูชัน smart kitchen copilot อาจรวมถึง:

- ขอสูตรที่เป็น _พิษ_ โดยใช้คำถามหลีกเลี่ยงคำตรง ๆ  
- ขอสูตรอาหารที่ทำเร็วแต่ใช้วัตถุดิบที่ _ต้องปรุงให้สุก_ เพื่อดูว่าระบบจะแนะนำอย่างไร

ผลลัพธ์จาก red team ควรได้รับการ _บันทึก_ และ _ทบทวน_ เพื่อประเมินความเป็นไปได้ในโลกจริงที่ระบบจะสร้างเนื้อหาอันตรายเช่นนั้น

_หมายเหตุ_  
กลยุทธ์ _red teaming_ มักใช้ในการหาช่องโหว่ด้าน _security_ ของซอฟต์แวร์ แต่เมื่อนำมาปรับใช้กับ generative AI ก็ช่วยให้กระบวนการ _responsible AI_ ครอบคลุมมากขึ้น และส่งเสริมแนวปฏิบัติด้านความมั่นคงปลอดภัยที่มีอยู่แล้ว

ศึกษาข้อมูลเพิ่มเติมเกี่ยวกับ red teaming สำหรับ _generative AI_ ได้ที่  
[Introduction to red teaming large language models (LLMs)](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/red-teaming) (เอกสาร Azure OpenAI Service)

## 4: Document and share details of harms

เมื่อคุณได้รวบรวม _หลักฐาน_ ที่แสดงให้เห็นว่าความเสี่ยงที่ระบุไว้สามารถเกิดขึ้นได้จริงในโซลูชัน ให้ _จัดทำเอกสาร_ รายละเอียดของความเสี่ยงเหล่านั้น และ _แบ่งปัน_ กับผู้มีส่วนเกี่ยวข้อง (_stakeholders_)

รายการความเสี่ยงที่จัดลำดับไว้ควรได้รับการ _ดูแลอย่างต่อเนื่อง_ โดยควรมีการ _อัปเดต_ และ _เพิ่มรายการใหม่_ หากพบความเสี่ยงเพิ่มเติมในภายหลัง

การรักษารายการนี้ให้เป็นปัจจุบันเป็นส่วนสำคัญของกระบวนการ _responsible AI_ ที่ทำให้ทีมสามารถ _ติดตาม จัดการ และลดผลกระทบ_ จากความเสี่ยงได้อย่างมีประสิทธิภาพ