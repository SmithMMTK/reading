
_Delta Lake_ คือ _open-source storage layer_ ที่เพิ่มความสามารถแบบ _relational database semantics_ ให้กับการประมวลผลข้อมูลใน _data lake_ บนฐานของ _Spark_

ใน Microsoft Fabric _tables_ ที่อยู่ใน _lakehouse_ ล้วนเป็น _Delta tables_ ซึ่งเราจะเห็นได้จากไอคอนรูปสามเหลี่ยม **Δ** ที่แสดงอยู่ข้างหน้าชื่อตารางในหน้า _lakehouse user interface_

![Screenshot of the salesorders table viewed in the Lakehouse explorer in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/delta-table.png)

_Delta tables_ เป็นโครงสร้าง _schema abstraction_ ที่ครอบอยู่บนไฟล์ข้อมูลที่จัดเก็บในรูปแบบ _Delta format_

สำหรับแต่ละ _table_ ใน _lakehouse_ จะมีโฟลเดอร์ที่เก็บไฟล์ข้อมูลชนิด _Parquet_ และมีโฟลเดอร์ **_delta_log** ซึ่งใช้บันทึกรายละเอียดของ _transaction_ ต่าง ๆ ในรูปแบบไฟล์ _JSON_

![Screenshot of the files view of the parquet files in the salesorders table viewed through Lakehouse explorer.](https://learn.microsoft.com/en-us/training/wwl/work-delta-lake-tables-fabric/media/delta-files.png)

ข้อดีของการใช้ _Delta tables_ ได้แก่:

- _tables_ แบบ _relational_ ที่รองรับการ _query_ และการแก้ไขข้อมูล ด้วย _Apache Spark_ คุณสามารถเก็บข้อมูลใน _Delta tables_ ที่รองรับการทำงานแบบ _CRUD_ (_create_, _read_, _update_, _delete_) ได้ หมายความว่าคุณสามารถ _select_, _insert_, _update_, และ _delete_ ข้อมูลได้เหมือนกับที่ทำในระบบฐานข้อมูลทั่วไป
- รองรับ _ACID transactions_ _Delta Lake_ นำคุณสมบัติ _atomicity_ (ธุรกรรมต้องเสร็จสมบูรณ์ทั้งชุด), _consistency_ (ทำให้ข้อมูลยังอยู่ในสถานะที่ถูกต้อง), _isolation_ (ธุรกรรมที่เกิดขึ้นพร้อมกันจะไม่รบกวนกัน), และ _durability_ (เมื่อเสร็จแล้วจะถูกบันทึกถาวร) มาใช้กับ _Spark_ โดยใช้ _transaction log_ และจัดการ _serializable isolation_ สำหรับการทำงานพร้อมกัน
- มีระบบ _data versioning_ และ _time travel_ เพราะ _transactions_ ทั้งหมดจะถูกบันทึกไว้ใน _transaction log_ ทำให้สามารถย้อนกลับไปดูเวอร์ชันก่อนหน้าของข้อมูลแต่ละแถว และใช้ฟีเจอร์ _time travel_ เพื่อ _query_ ข้อมูลในอดีตได้
- รองรับข้อมูลแบบ _batch_ และ _streaming_ ต่างจากฐานข้อมูลทั่วไปที่มักจัดเก็บข้อมูลแบบคงที่ _Delta Lake tables_ สามารถใช้ได้ทั้งกับ _batch processing_ และ _streaming_ โดยใช้เป็นทั้ง _source_ และ _sink_ ใน _Spark Structured Streaming API_
- ใช้ _standard formats_ และสามารถทำงานร่วมกับระบบอื่นได้ (_interoperability_) ข้อมูลใน _Delta tables_ จะถูกเก็บในรูปแบบ _Parquet_ ซึ่งเป็นมาตรฐานที่นิยมใช้ใน _data lake_ และยังสามารถใช้ _SQL analytics endpoint_ ของ Microsoft Fabric ในการ _query_ ตารางเหล่านี้ด้วย _SQL_ ได้อีกด้วย

