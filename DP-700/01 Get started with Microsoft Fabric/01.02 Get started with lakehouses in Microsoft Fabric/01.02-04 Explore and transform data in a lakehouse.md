
## Transform and load data

ข้อมูลส่วนใหญ่จำเป็นต้องมีการ _แปลงข้อมูล_ (_transformations_) ก่อนจะโหลดลงใน _tables_ โดยทั่วไปเราสามารถดึงข้อมูลดิบ (_raw data_) มาเก็บไว้ใน _lakehouse_ ก่อน แล้วจึงค่อยแปลงและโหลดลงใน _tables_ อีกที

ไม่ว่าคุณจะออกแบบ _ETL_ แบบไหน คุณสามารถใช้เครื่องมือชุดเดียวกับที่ใช้ดึงข้อมูลมาเพื่อแปลงและโหลดข้อมูลได้ง่าย ๆ โดยข้อมูลที่แปลงแล้วสามารถโหลดเป็น _file_ หรือ _Delta table_ ก็ได้

- _**Notebooks**_ เหมาะกับ _data engineers_ ที่เขียนโปรแกรมด้วยภาษาต่าง ๆ เช่น _PySpark_, _SQL_ หรือ _Scala_
- _**Dataflows Gen2**_ เหมาะกับผู้ที่คุ้นเคยกับ _Power BI_ หรือ _Excel_ เพราะใช้ _PowerQuery interface_ ซึ่งเป็นการลากวางและเขียนสูตรแบบง่าย
- _**Pipelines**_ มีอินเทอร์เฟซแบบภาพ (_visual interface_) ให้คุณออกแบบกระบวนการ _ETL_ ได้ตามต้องการ จะเรียบง่ายหรือซับซ้อนแค่ไหนก็ปรับได้ตามความเหมาะสม

## Analyze and visualize data in a lakehouse

เมื่อ _data_ ถูก _ingested_, _transformed_ และ _loaded_ เรียบร้อยแล้ว ข้อมูลก็พร้อมให้ผู้อื่นนำไปใช้งานต่อได้ _Fabric items_ จึงถูกออกแบบมาให้มีความยืดหยุ่น รองรับการทำงานของแต่ละบทบาทในองค์กรด้วยเครื่องมือที่เหมาะสม

- _**Data scientists**_ สามารถใช้ _notebooks_ หรือ _Data wrangler_ เพื่อสำรวจข้อมูลและฝึก _machine learning models_ สำหรับงาน _AI_
- _**Report developers**_ สามารถใช้ _semantic model_ เพื่อสร้างรายงานใน _Power BI_
- _**Analysts**_ สามารถใช้ _SQL analytics endpoint_ ในการ _query_, _filter_, _aggregate_ และวิเคราะห์ข้อมูลใน _lakehouse tables_

เมื่อรวมความสามารถในการสร้างภาพข้อมูล (_data visualization_) ของ _Power BI_ เข้ากับ _storage_ ที่รวมศูนย์และโครงสร้างตารางของ _data lakehouse_ จะทำให้คุณสามารถสร้างโซลูชัน _analytics_ ครบวงจรได้บนแพลตฟอร์มเดียว
