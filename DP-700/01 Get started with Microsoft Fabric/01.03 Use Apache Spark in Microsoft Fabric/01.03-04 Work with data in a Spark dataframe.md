
โดยพื้นฐานแล้ว Spark ใช้โครงสร้างข้อมูลที่เรียกว่า _resilient distributed dataset_ (_RDD_) แต่ถึงแม้เราจะ _สามารถ_ เขียนโค้ดเพื่อทำงานกับ _RDD_ ได้โดยตรง โครงสร้างข้อมูลที่นิยมใช้มากที่สุดในการจัดการกับข้อมูลแบบมีโครงสร้าง (_structured data_) ใน Spark คือ _dataframe_ ซึ่งมาพร้อมกับไลบรารี _Spark SQL_

_dataframe_ ใน Spark มีลักษณะคล้ายกับที่ใช้ในไลบรารี _Pandas_ ของภาษา Python ซึ่งเป็นที่รู้จักกันอย่างกว้างขวาง แต่ได้รับการปรับแต่งให้ทำงานได้ดีในสภาพแวดล้อมที่มีการประมวลผลแบบกระจายของ Spark

_Note_: นอกจาก _Dataframe API_ แล้ว _Spark SQL_ ยังมี _Dataset API_ ซึ่งเป็นแบบ _strongly-typed_ สำหรับใช้งานกับภาษา Java และ Scala ด้วย แต่ในเนื้อหานี้เราจะเน้นที่ _Dataframe API_ เป็นหลัก

## Loading data into a dataframe

ลองมาดูตัวอย่างสมมติที่จะช่วยให้เข้าใจวิธีใช้ _dataframe_ ในการทำงานกับข้อมูลได้ดียิ่งขึ้น สมมติว่าคุณมีข้อมูลในไฟล์ข้อความที่คั่นด้วยเครื่องหมายจุลภาค (_comma-delimited text file_) ชื่อว่า **products.csv** ซึ่งอยู่ในโฟลเดอร์ **Files/data** ภายใน _lakehouse_ ของคุณ

```csv
ProductID,ProductName,Category,ListPrice
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...
```

### Inferring a schema

คำสั่งนี้ใช้ฟังก์ชัน spark.read.csv() เพื่ออ่านไฟล์ CSV โดยกำหนดให้มี _header_ เป็นชื่อคอลัมน์ และให้ Spark เดา _schema_ ของข้อมูลให้อัตโนมัติ (_inferSchema=True_) จากนั้น df.show(10) จะแสดงข้อมูล 10 แถวแรกของ _dataframe_

```python
%%pyspark
df = spark.read.load('Files/data/products.csv',
    format='csv',
    header=True
)
display(df.limit(10))
```

บรรทัด `%%pyspark` ที่อยู่ด้านบนสุดของเซลล์เรียกว่า _magic_ ซึ่งใช้บอก Spark ว่าโค้ดในเซลล์นี้เขียนด้วยภาษา _PySpark_ คุณสามารถตั้งค่าภาษาเริ่มต้น (_default language_) ที่จะใช้ใน _notebook_ ได้จากแถบเครื่องมือ (_toolbar_) และสามารถใช้ _magic_ เพื่อเขียนเซลล์ใดเซลล์หนึ่งด้วยภาษาอื่นได้ชั่วคราว

ตัวอย่างเช่น ด้านล่างนี้คือโค้ด _Scala_ ที่มีผลเทียบเท่ากับตัวอย่างข้อมูล _products_:

```scala
%%spark
val df = spark.read.format("csv").option("header", "true").load("Files/data/products.csv")
display(df.limit(10))
```

_magic_ `%%spark` ใช้สำหรับระบุว่าโค้ดในเซลล์นั้นเขียนด้วยภาษา _Scala_

ทั้งตัวอย่างโค้ดของ _PySpark_ และ _Scala_ ด้านบนจะให้ผลลัพธ์ที่มีหน้าตาคล้ายกัน เช่น:

|ProductID|ProductName|Category|ListPrice|
|---|---|---|---|
|771|Mountain-100 Silver, 38|Mountain Bikes|3399.9900|
|772|Mountain-100 Silver, 42|Mountain Bikes|3399.9900|
|773|Mountain-100 Silver, 44|Mountain Bikes|3399.9900|
|...|...|...|...|
### Specifying an explicit schema

ในตัวอย่างก่อนหน้านี้ แถวแรกของไฟล์ _CSV_ มีชื่อคอลัมน์อยู่แล้ว และ Spark สามารถ _infer_ หรือเดาประเภทข้อมูลของแต่ละคอลัมน์ได้จากข้อมูลในไฟล์ แต่คุณก็สามารถกำหนด _schema_ ให้ชัดเจนได้เองเช่นกัน ซึ่งมีประโยชน์มากในกรณีที่ไฟล์ _CSV_ ไม่มีชื่อคอลัมน์ เช่นตัวอย่างไฟล์แบบนี้:

```csv
771,"Mountain-100 Silver, 38",Mountain Bikes,3399.9900
772,"Mountain-100 Silver, 42",Mountain Bikes,3399.9900
773,"Mountain-100 Silver, 44",Mountain Bikes,3399.9900
...
```

ตัวอย่าง _PySpark_ ด้านล่างนี้แสดงวิธีการกำหนด _schema_ อย่างชัดเจนสำหรับ _dataframe_ ที่โหลดมาจากไฟล์ชื่อ **product-data.csv** ซึ่งอยู่ในรูปแบบที่ไม่มีชื่อคอลัมน์:


```python
from pyspark.sql.types import *
from pyspark.sql.functions import *

productSchema = StructType([
    StructField("ProductID", IntegerType()),
    StructField("ProductName", StringType()),
    StructField("Category", StringType()),
    StructField("ListPrice", FloatType())
    ])

df = spark.read.load('Files/data/product-data.csv',
    format='csv',
    schema=productSchema,
    header=False)
display(df.limit(10))
```

ผลลัพธ์ที่ได้ก็จะคล้ายกับเดิมอีกครั้ง เช่น:

|ProductID|ProductName|Category|ListPrice|
|---|---|---|---|
|771|Mountain-100 Silver, 38|Mountain Bikes|3399.9900|
|772|Mountain-100 Silver, 42|Mountain Bikes|3399.9900|
|773|Mountain-100 Silver, 44|Mountain Bikes|3399.9900|
|...|...|...|...|

_Tip_: การกำหนด _schema_ อย่างชัดเจนช่วยให้ _ประสิทธิภาพดีขึ้น_ ด้วย!

## Filtering and grouping dataframes

คุณสามารถใช้เมธอดของคลาส _dataframe_ เพื่อ _filter_, _sort_, _group_ และจัดการข้อมูลภายในได้หลากหลายวิธี ตัวอย่างเช่น โค้ดด้านล่างนี้ใช้เมธอด `select` เพื่อดึงเฉพาะคอลัมน์ _ProductID_ และ _ListPrice_ จาก _dataframe_ ชื่อ **df** ที่เก็บข้อมูลสินค้า:

```python
pricelist_df = df.select("ProductID", "ListPrice")
```

ผลลัพธ์จากตัวอย่างโค้ดนี้จะมีหน้าตาคล้ายกับ:

|ProductID|ListPrice|
|---|---|
|771|3399.9900|
|772|3399.9900|
|773|3399.9900|
|...|...|

เหมือนกับเมธอดจัดการข้อมูลอื่น ๆ เมธอด **select** จะส่งคืนเป็น _dataframe_ ตัวใหม่ ไม่ได้แก้ไขข้อมูลต้นฉบับโดยตรง

_Tip_: การเลือกบางคอลัมน์จาก _dataframe_ เป็นสิ่งที่ทำบ่อย และสามารถเขียนด้วยรูปแบบสั้นลงได้แบบนี้:

`pricelist_df = df["ProductID", "ListPrice"]`

คุณสามารถ “_chain_” หรือเชื่อมต่อเมธอดหลายตัวเข้าด้วยกัน เพื่อดำเนินการกับข้อมูลแบบต่อเนื่อง และได้ _dataframe_ ใหม่ที่ถูกแปลงแล้ว
 
ตัวอย่างเช่น โค้ดด้านล่างนี้ใช้การเชื่อมเมธอด `select` และ `where` เพื่อนำข้อมูลเฉพาะ _ProductName_ และ _ListPrice_ ของสินค้าที่อยู่ในหมวด _Mountain Bikes_ หรือ _Road Bikes_:


```python
bikes_df = df.select("ProductName", "Category", "ListPrice").where((df["Category"]=="Mountain Bikes") | (df["Category"]=="Road Bikes"))
display(bikes_df)
```

ผลลัพธ์จากโค้ดตัวอย่างนี้จะมีหน้าตาคล้ายกับ:

|ProductName|Category|ListPrice|
|---|---|---|
|Mountain-100 Silver, 38|Mountain Bikes|3399.9900|
|Road-750 Black, 52|Road Bikes|539.9900|
|...|...|...|

หากต้องการจัดกลุ่ม (_group_) และรวมข้อมูล (_aggregate_) คุณสามารถใช้เมธอด **groupBy** ร่วมกับฟังก์ชันรวม เช่น `count`, `sum`, `avg` ได้
ตัวอย่าง _PySpark_ ด้านล่างนี้ใช้ `groupBy` เพื่อนับจำนวนสินค้าของแต่ละ _category_:


```python
counts_df = df.select("ProductID", "Category").groupBy("Category").count()
display(counts_df)
```

ผลลัพธ์จากโค้ดตัวอย่างนี้จะมีหน้าตาคล้ายกับ:

|Category|count|
|---|---|
|Headsets|3|
|Wheels|14|
|Mountain Bikes|32|
|...|...|

## Saving a dataframe

ในหลายกรณี คุณจะใช้ Spark เพื่อ _แปลงข้อมูลดิบ_ แล้ว _บันทึกผลลัพธ์_ เพื่อใช้ในการวิเคราะห์เพิ่มเติมหรือส่งต่อไปยังขั้นตอนถัดไป (_downstream processing_)
 
ตัวอย่างโค้ดด้านล่างนี้แสดงการบันทึก _dataframe_ ลงเป็นไฟล์ในรูปแบบ _parquet_ บน _data lake_ โดยจะเขียนทับไฟล์เดิมหากมีชื่อเดียวกันอยู่แล้ว:

```python
bikes_df.write.mode("overwrite").parquet('Files/product_data/bikes.parquet')
```


_Note_: รูปแบบไฟล์ _Parquet_ มักเป็นตัวเลือกที่แนะนำสำหรับการจัดเก็บข้อมูลที่ต้องใช้ในการวิเคราะห์ต่อ หรือโหลดเข้าสู่ระบบวิเคราะห์ข้อมูลในภายหลัง (_analytical store_)

_Parquet_ เป็นฟอร์แมตที่ _มีประสิทธิภาพสูง_ ทั้งในด้านการบีบอัดข้อมูลและการอ่านเฉพาะคอลัมน์ที่ต้องการ และยังได้รับการรองรับจากระบบวิเคราะห์ข้อมูลขนาดใหญ่เกือบทุกระบบ ในบางกรณี งานแปลงข้อมูล (_data transformation_) ของคุณอาจมีเพียงแค่การแปลงข้อมูลจากฟอร์แมตอื่น เช่น _CSV_ ให้กลายเป็น _Parquet_ ก็ถือว่ามีประโยชน์อย่างมากแล้ว

### Partitioning the output file

การ _partitioning_ เป็นเทคนิคการปรับประสิทธิภาพที่ช่วยให้ Spark ใช้ทรัพยากรของแต่ละ _worker node_ ได้เต็มที่ และยังช่วยเพิ่ม _performance_ ได้อีกมากเมื่อมีการกรองข้อมูล (_filtering_) เพราะจะช่วยลดการอ่านไฟล์ที่ไม่จำเป็น (_unnecessary disk IO_)

หากต้องการบันทึก _dataframe_ เป็นชุดไฟล์ที่มีการ _partition_ สามารถใช้เมธอด **partitionBy** ขณะเขียนข้อมูลได้

ตัวอย่างด้านล่างนี้จะแสดงการบันทึก _dataframe_ ชื่อ **bikes_df** ซึ่งมีข้อมูลสินค้าประเภท _mountain bikes_ และ _road bikes_ โดยจะ _partition_ ตาม _category_:

```python
bikes_df.write.partitionBy("Category").mode("overwrite").parquet("Files/bike_data")
```

โฟลเดอร์ที่สร้างขึ้นเมื่อมีการ _partition_ _dataframe_ จะมีชื่อในรูปแบบ _**column=value**_ โดยใช้ชื่อคอลัมน์และค่าของพาร์ติชันเป็นส่วนหนึ่งของชื่อโฟลเดอร์

ดังนั้นจากตัวอย่างโค้ดก่อนหน้านี้ จะเกิดโฟลเดอร์ชื่อ **bike_data** ที่ภายในมีโฟลเดอร์ย่อยแบบนี้:

- **Category=Mountain Bikes**
- **Category=Road Bikes**

```
bike_data/
├── Category=Mountain Bikes/
│   └── part-00000-…
├── Category=Road Bikes/
│   └── part-00001-…
```

แต่ละโฟลเดอร์ย่อยจะมีไฟล์ _parquet_ อย่างน้อยหนึ่งไฟล์ ซึ่งเก็บข้อมูลสินค้าตาม _category_ ที่เหมาะสม

_Note_: คุณสามารถ _partition_ ข้อมูลด้วยหลายคอลัมน์ได้เช่นกัน ซึ่งจะทำให้เกิดโครงสร้างโฟลเดอร์แบบ _ลำดับชั้น_ ตามคีย์ที่ใช้ _partition_ ตัวอย่างเช่น หากคุณมีข้อมูลคำสั่งซื้อ (_sales order_) และต้องการแบ่งตาม _ปี_ และ _เดือน_ ระบบจะสร้างโฟลเดอร์แบบนี้:

## Load partitioned data

เมื่อคุณต้องการโหลดข้อมูลที่ถูก _partition_ ไว้กลับมาใช้งานในรูปแบบ _dataframe_ คุณสามารถระบุโฟลเดอร์ที่ต้องการอ่านได้โดยใช้ค่าที่เจาะจงหรือใช้ _wildcard_ สำหรับคอลัมน์ที่ถูก _partition_

ตัวอย่างด้านล่างนี้แสดงการโหลดข้อมูลสินค้าที่อยู่ใน _category_ **Road Bikes**:

```python
road_bikes_df = spark.read.parquet('Files/bike_data/Category=Road Bikes')
display(road_bikes_df.limit(5))
```

_Note_: คอลัมน์ที่ใช้ในการ _partition_ ซึ่งถูกระบุไว้ในพาธของไฟล์ **จะไม่แสดงอยู่ใน _dataframe_ ที่โหลดมา** ข้อมูลที่ได้ใน _dataframe_ จะไม่มีคอลัมน์ _Category_ ปรากฏอยู่ แต่ค่าของ _Category_ สำหรับทุกแถวใน _dataframe_ นี้คือ _Road Bikes_ ทั้งหมด