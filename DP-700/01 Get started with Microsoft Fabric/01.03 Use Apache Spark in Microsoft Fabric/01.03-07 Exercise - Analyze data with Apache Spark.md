
[Launch Exercise](https://go.microsoft.com/fwlink/?linkid=2259707)

## Create a DataFrame

```python
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
# df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
display(df)
```

```output
### Sales Orders Summary

| SO43704     | 1        | 2019-07-01 | Julio Ruiz      | ...
|-------------|----------|------------|-----------------|----- 
| SO43705     | 1        | 2019-07-01 | Curtis Lu       | ...
| SO43700     | 1        | 2019-07-01 | Ruben Prasad    | ...
| SO43703     | 1        | 2019-07-01 | Albert Alvarez  | ...
| SO43697     | 1        | 2019-07-01 | Cole Watson     | ...
| SO43699     | 1        | 2019-07-01 | Sydney Wright   | ...
...
```


```python
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
```

```output
### Sales Orders Summary
| _c0         | _c1      | _c2        | _c3             | ...
|-------------|----------|------------|-----------------|----- 
| SO43704     | 1        | 2019-07-01 | Julio Ruiz      | ...
| SO43705     | 1        | 2019-07-01 | Curtis Lu       | ...
| SO43700     | 1        | 2019-07-01 | Ruben Prasad    | ...
| SO43703     | 1        | 2019-07-01 | Albert Alvarez  | ...
| SO43697     | 1        | 2019-07-01 | Cole Watson     | ...
| SO43699     | 1        | 2019-07-01 | Sydney Wright   | ...
...
```

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")

display(df)
```

```output
### Sales Orders Summary
| OrderNumber | Quantity | OrderDate  | SalesRep        | ...
|-------------|----------|------------|-----------------|----- 
| SO43704     | 1        | 2019-07-01 | Julio Ruiz      | ...
| SO43705     | 1        | 2019-07-01 | Curtis Lu       | ...
| SO43700     | 1        | 2019-07-01 | Ruben Prasad    | ...
| SO43703     | 1        | 2019-07-01 | Albert Alvarez  | ...
| SO43697     | 1        | 2019-07-01 | Cole Watson     | ...
| SO43699     | 1        | 2019-07-01 | Sydney Wright   | ...
...
```

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")

display(df)
```

## Explore data in a DataFrame

```python
customers = df['CustomerName', 'Email']

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

```output
### Customer Contact List

| CustomerName       | Email                    |
|--------------------|--------------------------|
| Bridget Andersen   | bridget15@...            |
| Mya Butler         | mya14@...                |
| Deanna Hernandez   | deanna29@...             |
| Ricky Navarro      | ricky10@...              |
| Omar Ye            | omar9@...                |
| Kollie Gutierrez   | kollie9@...              |
```

```python
customers = df.select("CustomerName", "Email")
```

```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

### Aggregate and group data in a DataFrame

```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()

display(productSales)
```

```output
### Total Quantity by Item

| Item                          | sum(Quantity) |
|-------------------------------|----------------|
| Mountain-200 Black, 42        | 388            |
| Touring-1000 Yellow, 46       | 74             |
| Touring-1000 Blue, 54         | 67             |
| Short-Sleeve Classic Jersey, S| 216            |
| Women's Mountain Shorts, S    | 146            |
| Long-Sleeve Logo Jersey, L    | 234            |
| Mountain-400-W Silver, 42     | 59             |
```

```python
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")

display(yearlySales)
```

```output
### Record Count by Year

| Year | Count |
|------|-------|
| 2019 | 1201  |
| 2020 | 2733  |
| 2021 | 28784 |
```

## Use Spark to transform data files

```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create the new FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]

# Display the first five orders
display(transformed_df.limit(5))
```

```output
### Sales Order Records

| SalesOrder   | SalesOrder| OrderDate  | Year | Month | FirstName | LastName
| Number       | LineNumber|            |      |       |           |
|--------------|-----------|------------|------|-------|-----------|---------
| SO49171      | 1         | 2021-01-01 | 2021 | 1     | Mariah    | Foster     
| SO49172      | 1         | 2021-01-01 | 2021 | 1     | Brian     | Howard     
| SO49173      | 1         | 2021-01-01 | 2021 | 1     | Linda     | Alvarez    
| SO49174      | 1         | 2021-01-01 | 2021 | 1     | Gina      | Hernandez  
| SO49178      | 1         | 2021-01-01 | 2021 | 1     | Beth      | Ruiz       
```

```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')

print ("Transformed data saved!")
```

```output
### DeltaTableLakehouse

#### Files

- transformed_data/
  └── orders/
      ├── _SUCCESS
      ├── part-00000-...-c000.snappy.parquet
      ├── part-00001-...-c000.snappy.parquet
      └── part-00002-...-c000.snappy.parquet
```

```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

```output
### Sales Order Records

| SalesOrder   | SalesOrder| OrderDate  | Year | Month | FirstName | LastName
| Number       | LineNumber|            |      |       |           |
|--------------|-----------|------------|------|-------|-----------|---------
| SO49171      | 1         | 2021-01-01 | 2021 | 1     | Mariah    | Foster     
| SO49172      | 1         | 2021-01-01 | 2021 | 1     | Brian     | Howard     
| SO49173      | 1         | 2021-01-01 | 2021 | 1     | Linda     | Alvarez    
| SO49174      | 1         | 2021-01-01 | 2021 | 1     | Gina      | Hernandez  
| SO49178      | 1         | 2021-01-01 | 2021 | 1     | Beth      | Ruiz      
| SO49179      | 1         | 2021-01-01 | 2021 | 1     | Evan      | Ward      
| SO49175      | 1         | 2021-01-01 | 2021 | 1     | Margaret  | Guo   
```

```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")

print ("Transformed data saved!")
```

```output
### DeltaTableLakehouse

#### Files

- partitioned_data/
  ├── Year=2019/
  │   ├── Month=7/
  │   ├── Month=8/
  │   ├── Month=9/
  │   │   └── part-00002-a35d21f1-719a-492d-8376-4387faa71280.c000.snappy.parquet
  │   ├── Month=10/
  │   ├── Month=11/
  │   └── Month=12/
  ├── Year=2020/
  └── Year=2021/
```

```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)
```

## Work with tables and SQL

```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

```python
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")

display(df)
```

```output
### Sales Order Records

| SalesOrder | SalesOrder | OrderDate  | Year | Month | FirstName | LastName  |
| Number     | LineNumber |            |      |       |           |           |
|------------|------------|------------|------|--------|-----------|-----------|
| SO49171    | 1          | 2021-01-01 | 2021 | 1      | Mariah    | Foster    |
| SO49172    | 1          | 2021-01-01 | 2021 | 1      | Brian     | Howard    |
| SO49173    | 1          | 2021-01-01 | 2021 | 1      | Linda     | Alvarez   |
| SO49174    | 1          | 2021-01-01 | 2021 | 1      | Gina      | Hernandez |
| SO49178    | 1          | 2021-01-01 | 2021 | 1      | Beth      | Ruiz      |
...
```

```sql
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

```output
### Gross Revenue by Year

| OrderYear | GrossRevenue      |
|-----------|-------------------|
| 2019      | 4172169.969970703 |
| 2020      | 6882259.268127441 |
| 2021      | 11547835.2916965  |
```

```sql
%%sql
SELECT * FROM salesorders
```

select **+ New chart**

```output
### Bar Chart (Sum of Quantity by Item)

Mountain-100 Black, 38     | █████                         (10)
Mountain-100 Black, 42     | ██████████                    (18)
Mountain-100 Black, 44     | ████████████                  (22)
Mountain-100 Black, 48     | ██████████████████           (36)

Mountain-100 Silver, 38    | ██████████                    (18)
Mountain-100 Silver, 42    | ██████                        (12)
Mountain-100 Silver, 44    | ███████                       (14)
Mountain-100 Silver, 48    | ████████                      (16)

Road-150 Red, 44           | ██████████████████████████    (110)
Road-150 Red, 48           | █████████████████████████████ (150)
Road-150 Red, 52           | █████████████████████████     (135)
Road-150 Red, 56           | ██████████████████████        (120)
Road-150 Red, 62           | ████████████████████████      (140)

Road-650 Black, 44         | █████                         (10)
Road-650 Black, 48         | ███████                       (15)
Road-650 Black, 52         | █████████                     (20)
Road-650 Black, 58         | ████████                      (18)
Road-650 Black, 60         | ███████                       (15)
Road-650 Black, 62         | ██████████                    (22)

Road-650 Red, 44           | ███████                       (18)
Road-650 Red, 48           | ██████                        (16)
Road-650 Red, 52           | █████                         (14)
Road-650 Red, 56           | ██████                        (16)
Road-650 Red, 60           | ███████                       (18)
Road-650 Red, 62           | █████████                     (20)
```