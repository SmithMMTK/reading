
[Launch Exercise](https://go.microsoft.com/fwlink/?linkid=2259245)


## Explore data in a DataFrame

```python
from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

# define the schema
schema = StructType() \
.add("ProductID", IntegerType(), True) \
.add("ProductName", StringType(), True) \
.add("Category", StringType(), True) \
.add("ListPrice", DoubleType(), True)

df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")

# df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
display(df)
```

```output
| ProductID | ProductName         | Category       | ListPrice |
|-----------|---------------------|----------------|-----------|
| 771       | Mountain-100 Silver | Mountain Bikes | 3399.99   |
| 772       | Mountain-100 Silver | Mountain Bikes | 3399.99   |
| 773       | Mountain-100 Silver | Mountain Bikes | 3399.99   |
| 774       | Mountain-100 Silver | Mountain Bikes | 3399.99   |
...
...
...
```

## Create Delta tables

```python
df.write.format("delta").saveAsTable("managed_products")
```

```output
Items
└── DeltaTableLakehouse
    ├── Tables
    │   └── managed_products (Delta Table)
    └── Files
        └── products
```

### Create an external table


```python

df.write.format("delta").saveAsTable("external_products", path="abfss://e0303e8e-4d45-4722-9d32-3794334e3819@onelake.dfs.fabric.microsoft.com/d0940276-3cb1-4f4e-81a6-d8407b1b7594/Files/external_products")
```

```output
Items
└── DeltaTableLakehouse
    ├── Tables
    │   ├── external_products (Delta Table)
    │   └── managed_products (Delta Table)
    └── Files
        ├── external_products
        │   ├── _delta_log/
        │   └── part-00000-05f96027-98d6-4030-... (Parquet file)
        └── products
```

### Compare managed and external tables

```sql
%%sql
DESCRIBE FORMATTED managed_products;
```

```output

Table: managed_products:

| Column Name  | Data Type |
|--------------|-----------|
| ProductID    | int       |
| ProductName  | string    |
| Category     | string    |
| ListPrice    | double    |

Detailed Table Information:

| Property           | Value                                                                
|--------------------|-----------------------------------------------------
| Name               | spark_catalog.deltatablelakehouse.managed_products
| Type               | MANAGED      
| Location           | abfss://.../Tables/managed_products
| Provider           | delta
| Owner              | trusted-service-user
| Table Properties   | [delta.minReaderVersion=1, delta.minWriterVersion=2,
|                    |  delta.stats.extended.collect=true,
|                    |  delta.stats.extended.inject=true]
```

```sql
%%sql
DESCRIBE FORMATTED external_products;
```

```output
Table: external_products

| Column Name  | Data Type |
|--------------|-----------|
| ProductID    | int       |
| ProductName  | string    |
| Category     | string    |
| ListPrice    | double    |

Detailed Table Information

| Property           | Value                                                                
|--------------------|-----------------------------------------------------
| Name               | spark_catalog.deltatablelakehouse.external_products                  
| Type               | EXTERNAL         
| Location           | abfss://.../Files/external_products    
| Provider           | delta        
| Owner              | trusted-service-user        
| Table Properties   | [delta.minReaderVersion=1, delta.minWriterVersion=2,   
|                    |  delta.stats.extended.collect=true,     
|                    |  delta.stats.extended.inject=true]     
```

```sql
%%sql
DROP TABLE managed_products;
DROP TABLE external_products;
```
*Note:* The data in Files folder still remains

## Use SQL to create a Delta table

```sql
%%sql
CREATE TABLE products
USING DELTA
LOCATION 'Files/external_products';
```

```sql
%%sql 
SELECT * FROM products;
```


## Explore table versioning

```sql
%%sql
UPDATE products
SET ListPrice = ListPrice * 0.9
WHERE Category = 'Mountain Bikes';
```

```sql
%%sql
DESCRIBE HISTORY products;
```

```output
### Delta Table History

| version | timestamp           | operation     | operationParameters   
|---------|---------------------|---------------|-----------------------
| 1       | 2025-07-25T04:...   |   UPDATE      | {"predicate":"[\"(Category#..."]}       
| 0       | 2025-07-25T04:...   | CREATE TABLE  | {"partitionBy":[],"clusteredBy":
```

```python
delta_table_path = 'Files/external_products'
# Get the current data
current_data = spark.read.format("delta").load(delta_table_path)
display(current_data)

# Get the version 0 data
original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
display(original_data)
```

Show the difference
```python
delta_table_path = 'Files/external_products'

# Load current and original (version 0) data
current_data = spark.read.format("delta").load(delta_table_path)
original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)

# Differences: added or modified rows
added_or_modified = current_data.exceptAll(original_data)

# Differences: removed rows
removed = original_data.exceptAll(current_data)

# Display the differences
print("Added or Modified Rows:")
display(added_or_modified)

print("Removed Rows:")
display(removed)
```

## Analyze Delta table data with SQL queries

```sql
%%sql
-- Create a temporary view
CREATE OR REPLACE TEMPORARY VIEW products_view
AS
    SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
    FROM products
    GROUP BY Category;

SELECT *
FROM products_view
ORDER BY Category;    
```

```sql
%%sql
SELECT Category, NumProducts
FROM products_view
ORDER BY NumProducts DESC
LIMIT 10;
```

```python
from pyspark.sql.functions import col, desc

df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
display(df_products.limit(6))
```

