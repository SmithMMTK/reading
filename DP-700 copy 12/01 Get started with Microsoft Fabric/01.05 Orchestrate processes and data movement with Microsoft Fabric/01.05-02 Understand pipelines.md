
# Understand pipelines

_Pipelines_ ใน _Microsoft Fabric_ จะประกอบด้วยชุดของ _activities_ ที่ทำหน้าที่เคลื่อนย้ายและประมวลผลข้อมูล คุณสามารถใช้ _pipeline_ เพื่อกำหนดกิจกรรมการโอนถ่ายและแปลงข้อมูล และควบคุมลำดับเหล่านี้ผ่าน _control flow activities_ ที่จัดการ _branching_, _looping_ และตรรกะการประมวลผลทั่วไปอื่น ๆ

พื้นที่ทำงานแบบกราฟิก (_pipeline canvas_) ใน _Fabric user interface_ ช่วยให้คุณสามารถสร้าง _pipelines_ ที่ซับซ้อนได้ โดยแทบไม่ต้องเขียนโค้ด

![Screenshot of a pipeline in Microsoft Fabric.](https://learn.microsoft.com/en-us/training/wwl/use-data-factory-pipelines-fabric/media/pipeline.png)

## Core pipeline concepts

ก่อนจะเริ่มสร้าง _pipelines_ ใน _Microsoft Fabric_ คุณควรเข้าใจแนวคิดหลักเหล่านี้ก่อน

### Activities

_Activities_ คือชุดคำสั่งที่สามารถทำงานได้จริงภายใน _pipeline_ โดยคุณสามารถกำหนดลำดับของ _activities_ ผ่านการเชื่อมโยงกันเป็นสายงาน โดยผลลัพธ์ของแต่ละ _activity_ (เช่น สำเร็จ ล้มเหลว หรือเสร็จสมบูรณ์) จะถูกใช้เพื่อนำทางไปยัง _activity_ ถัดไปในลำดับ

ใน _pipeline_ จะมี _activities_ อยู่ 2 กลุ่มใหญ่ ๆ:

- **Data transformation activities** - คือกิจกรรมที่เกี่ยวข้องกับการถ่ายโอนข้อมูล เช่น **Copy Data** ที่ใช้ดึงข้อมูลจากต้นทางและโหลดไปยังปลายทาง และ **Data Flow** ที่ทำหน้าที่แปลงข้อมูลระหว่างทางผ่าน _dataflows (Gen2)_ นอกจากนี้ยังมีกิจกรรมประเภท **Notebook** สำหรับรัน _Spark notebook_, **Stored procedure** สำหรับรันคำสั่ง _SQL_, **Delete data** เพื่อใช้ลบข้อมูลเดิม และอื่น ๆ โดยสามารถกำหนดปลายทางของข้อมูลใน _OneLake_ ให้เป็น _lakehouse_, _warehouse_, _SQL database_ หรือปลายทางอื่น ๆ ได้

- **Control flow activities** - คือกิจกรรมที่ใช้ควบคุมลำดับ เช่น การวนลูป การแตกแขนงเงื่อนไข หรือจัดการค่าของตัวแปรและพารามิเตอร์ ซึ่งช่วยให้คุณสามารถสร้างลำดับการทำงานที่ซับซ้อนเพื่อควบคุมการนำเข้าและแปลงข้อมูลได้อย่างยืดหยุ่น

_Tip_
สำหรับรายละเอียดทั้งหมดเกี่ยวกับ _activities_ ที่มีให้ใช้ใน _Microsoft Fabric_ ดูเพิ่มเติมที่ [Activity overview](https://learn.microsoft.com/en-us/fabric/data-factory/activity-overview) ในเอกสาร Microsoft Fabric

### Parameters

_Pipelines_ สามารถตั้งค่าให้รับ _parameters_ ได้ ซึ่งช่วยให้คุณสามารถระบุค่าที่เฉพาะเจาะจงสำหรับแต่ละครั้งที่ _pipeline_ ถูกรัน ตัวอย่างเช่น คุณอาจต้องการให้ _pipeline_ บันทึกข้อมูลที่นำเข้าไว้ในโฟลเดอร์ โดยสามารถระบุชื่อโฟลเดอร์ใหม่ได้ทุกครั้งที่รัน

การใช้ _parameters_ ทำให้ _pipeline_ สามารถนำกลับมาใช้ซ้ำได้ง่าย และสร้างกระบวนการที่ยืดหยุ่นสำหรับการนำเข้าและแปลงข้อมูล

### Pipeline runs

ทุกครั้งที่ _pipeline_ ถูกรัน จะมีการเริ่มต้น _data pipeline run_ ขึ้นมา ซึ่งอาจถูกรันแบบออนดีมานด์ผ่าน _Fabric user interface_ หรือกำหนดเวลาให้ทำงานตามช่วงเวลาที่ตั้งไว้ก็ได้ คุณสามารถใช้ _run ID_ เฉพาะในการตรวจสอบผลการรัน ตรวจสอบว่าเสร็จสมบูรณ์หรือไม่ และดูค่าการตั้งค่าเฉพาะที่ใช้ในการรันแต่ละครั้ง