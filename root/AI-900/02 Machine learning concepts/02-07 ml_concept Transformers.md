[[04-04 Understand how transformers advance language models]]
# Transformers

โมเดล AI สมัยใหม่ที่ใช้สร้างข้อความ เช่น ChatGPT หรือเครื่องมือสรุปข้อความ ล้วนขับเคลื่อนโดย *language models* ซึ่งเป็นโมเดล machine learning ชนิดพิเศษ ที่ใช้สำหรับ *Natural Language Processing (NLP)* โดยเฉพาะ

ตัวอย่างงานที่สามารถทำได้ด้วย language model ได้แก่:

- วิเคราะห์อารมณ์หรือจำแนกข้อความ (Sentiment & Text Classification)  
- สรุปข้อความ (Summarization)  
- เปรียบเทียบข้อความหลายชุดเพื่อวิเคราะห์ความหมาย (Semantic Similarity)  
- สร้างข้อความใหม่ (Text Generation)

แม้หลักการคณิตศาสตร์เบื้องหลังโมเดลเหล่านี้จะซับซ้อน  
แต่การเข้าใจ *architecture* ที่ใช้ (เช่น **transformer**) จะช่วยให้เราเข้าใจการทำงานของมันได้ดีขึ้นในเชิงแนวคิด

## Transformer Models

โมเดล machine learning สำหรับ *Natural Language Processing (NLP)* ได้พัฒนามาอย่างต่อเนื่องหลายปี โดยโมเดลที่ล้ำหน้าที่สุดในปัจจุบันคือ *transformer-based models*  ซึ่งเป็นสถาปัตยกรรมที่ต่อยอดจากเทคนิคเดิมในการเรียนรู้คำศัพท์และโครงสร้างภาษา

โมเดล *transformer* ถูกฝึกด้วยข้อมูลข้อความขนาดใหญ่ ทำให้สามารถเรียนรู้ *semantic relationships* หรือความสัมพันธ์เชิงความหมายระหว่างคำ และนำความสัมพันธ์เหล่านี้ไปใช้สร้างลำดับข้อความที่เป็นธรรมชาติและสมเหตุสมผล

เมื่อมีขนาดใหญ่เพียงพอ (*Large Language Model - LLM*)  โมเดลเหล่านี้สามารถสร้างข้อความที่แทบแยกไม่ออกจากสิ่งที่มนุษย์เขียน

### สถาปัตยกรรมของ Transformer แบ่งออกเป็น 2 ส่วนหลัก:

- **Encoder**: แปลงข้อความต้นทางให้เป็นตัวแทนเชิงความหมาย (*semantic representation*)  
- **Decoder**: ใช้ตัวแทนเชิงความหมายที่ได้มาเพื่อสร้างลำดับข้อความใหม่

![Diagram of transformer model architecture with the encoder and decoder blocks.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/transformer-model.png)

### How Transformer Models Work

โมเดล transformer ถูกฝึกด้วยข้อมูลข้อความธรรมชาติ (natural language) จำนวนมหาศาล ซึ่งมักได้มาจากอินเทอร์เน็ตหรือแหล่งข้อมูลสาธารณะอื่น ๆ

#### ขั้นตอนการประมวลผล:

1. **Tokenization**  
   ข้อความจะถูกแบ่งออกเป็น *tokens* (เช่น คำแต่ละคำ หรือ subword)

2. **Encoding with Attention**  
   *Encoder block* จะประมวลผลลำดับ token ด้วยเทคนิคที่เรียกว่า *attention* เพื่อเรียนรู้ว่าแต่ละ token มีความสัมพันธ์ กับ token อื่นอย่างไร เช่น token ใดมีอิทธิพลต่อกัน หรือมักปรากฏร่วมกันในบริบทเดียวกัน

3. **Embeddings**  
   ผลลัพธ์จาก encoder คือเวกเตอร์หลายมิติ (vector) สำหรับแต่ละ token โดยในแต่ละเวกเตอร์ประกอบด้วยค่าที่แทนคุณสมบัติทางความหมายของคำนั้น เวกเตอร์เหล่านี้เรียกว่า *embeddings*

4. **Decoding to Generate Output**  
   *Decoder block* จะรับลำดับ token ใหม่ (เช่น ข้อความที่กำลังจะถูกเติม) แล้วใช้ *embeddings* ที่ได้จาก encoder เพื่อสร้าง output ที่เหมาะสมในรูปแบบภาษาธรรมชาติ

#### ตัวอย่าง:

หาก input คือ:  **"When my dog was"**

โมเดลจะใช้ attention วิเคราะห์บริบทของคำในประโยค รวมถึงข้อมูล semantic จาก embeddings แล้วสร้างข้อความต่อเช่น:  **"a puppy"**

ทำให้ประโยคสมบูรณ์และเป็นธรรมชาติ

ในการใช้งานจริง สถาปัตยกรรมของ transformer สามารถปรับเปลี่ยนได้ตามวัตถุประสงค์ของโมเดล  
ตัวอย่างที่ชัดเจน ได้แก่:

- **BERT** (*Bidirectional Encoder Representations from Transformers*) พัฒนาโดย Google เพื่อใช้ในการเข้าใจความหมายของข้อความ เช่น การค้นหาข้อมูล ใช้ **เฉพาะ encoder block** เพื่อวิเคราะห์ความสัมพันธ์ของคำจากทั้งสองทิศทางในประโยค

- **GPT** (*Generative Pretrained Transformer*) พัฒนาโดย OpenAI ใช้สำหรับการ *สร้างภาษา (text generation)*  
  ใช้ **เฉพาะ decoder block** เพื่อสร้างข้อความใหม่ทีละ token โดยอิงจากบริบทก่อนหน้า

> หมายเหตุ: GPT ใช้ *causal attention* เพื่อไม่ให้เห็นคำในอนาคตระหว่างการสร้างข้อความ

แม้ว่าการอธิบายรายละเอียดทั้งหมดของ transformer จะเกินขอบเขตของบทนี้ แต่การเข้าใจองค์ประกอบสำคัญบางส่วนของมันจะช่วยให้เห็นภาพว่า โมเดลประเภทนี้สามารถสนับสนุนการทำงานของ *generative AI* ได้อย่างไร

### Tokenization

ขั้นตอนแรกของการฝึกโมเดล *transformer* คือการแยกข้อความออกเป็น *tokens* กล่าวคือ การระบุหน่วยข้อความที่มีความหมายเฉพาะตัว

เพื่อความเข้าใจง่าย ให้ถือว่าแต่ละคำในประโยคคือตัวอย่างของ token (แม้ในความเป็นจริง token อาจเป็นคำบางส่วน หรือรวมคำกับเครื่องหมายวรรคตอนด้วย)

#### ตัวอย่าง:

ข้อความ:  
**"I heard a dog bark loudly at a cat"**

สามารถแบ่งออกเป็น token ดังนี้:  
**["I", "heard", "a", "dog", "bark", "loudly", "at", "a", "cat"]**

แล้วกำหนด *token IDs* ให้แต่ละคำ เช่น:

```text
- I (1)
- heard (2)
- a (3)
- dog (4)
- bark (5)
- loudly (6)
- at (7)
- ("a" is already tokenized as 3)
- cat (8)
```

ข้อความ **"I heard a dog bark loudly at a cat"** สามารถแทนด้วยลำดับ token IDs ได้ดังนี้: **{1 2 3 4 5 6 7 3 8}** และข้อความ **"I heard a cat"**  จะกลายเป็น:  **{1 2 3 8}**

เมื่อฝึกโมเดลต่อไปเรื่อย ๆ  ทุก token ใหม่ที่พบในข้อมูลฝึกจะถูกเพิ่มเข้าไปใน *vocabulary* โดยกำหนดหมายเลข *token ID* ที่ไม่ซ้ำกับคำก่อนหน้า

```text
- meow (9)
- skateboard (10)
- *and so on...*
```

เมื่อใช้ข้อมูลฝึกที่มีขนาดใหญ่เพียงพอ โมเดลจะสามารถสร้าง *vocabulary* ที่ประกอบด้วย token จำนวนหลายพันรายการขึ้นไป

*vocabulary* นี้จะครอบคลุมคำที่ใช้บ่อย คำเฉพาะทาง คำบางส่วน (*subwords*) และแม้กระทั่งเครื่องหมายวรรคตอนหรือคำที่มักปรากฏร่วมกัน

การมี vocabulary ขนาดใหญ่ช่วยให้โมเดลสามารถเข้าใจและสร้างภาษาที่หลากหลายและแม่นยำ แต่ก็เพิ่มขนาดของโมเดลและความซับซ้อนในการประมวลผลเช่นกัน

### Embeddings

การแทน token ด้วยหมายเลข ID เป็นเพียงการสร้างดัชนี (index) แต่มันไม่สามารถบอกความหมาย หรือความสัมพันธ์ระหว่างคำได้ เพื่อให้โมเดลเข้าใจความหมายและบริบทของคำ เราจึงใช้ **embedding vectors** เพื่อแทนแต่ละ token โดย embedding คือเวกเตอร์เชิงตัวเลขหลายมิติ (เช่น [10, 3, 1]) ซึ่งแต่ละมิติแทนคุณลักษณะเชิงความหมายบางอย่างของคำนั้น

#### ลักษณะของเวกเตอร์:

- เวกเตอร์คือเส้นตรงใน *space* หลายมิติ แสดงทั้ง *ทิศทาง* (direction) และ *ระยะทาง* (magnitude) จากจุดเริ่มต้น
- ยิ่งคำสองคำมีความหมายใกล้กัน เวกเตอร์ของพวกมันจะชี้ไปในทิศทางใกล้เคียงกัน (หรือมี *orientation* ที่ใกล้กันใน embedding space)
- วิธีวัดความใกล้เคียงของเวกเตอร์คือ *cosine similarity* ซึ่งดูว่าเวกเตอร์สองเส้นชี้ไปในทิศทางเดียวกันหรือไม่ โดยไม่สนใจว่าระยะทางยาวแค่ไหน

```text
- 4 ("dog"): [10,3,2]
- 8 ("cat"): [10,3,1]
- 9 ("puppy"): [5,2,1]
- 10 ("skateboard"): [-3,3,2]
```

เราสามารถนำ embedding vectors มาแสดงในกราฟแบบสามมิติ

![Diagram of token vectors plotted in three dimensional space.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/embed-example.png)

### Interpreting Embedding Vectors

จากตัวอย่างก่อนหน้า เวกเตอร์ของคำว่า **"dog"** และ **"puppy"** มีทิศทางเกือบเหมือนกันใน embedding space และยังอยู่ใกล้กับคำว่า **"cat"** ซึ่งมีความหมายใกล้เคียงกัน (สัตว์เลี้ยง) ในทางกลับกัน คำว่า **"skateboard"** มีเวกเตอร์ที่ชี้ไปในทิศทางต่างออกไปมาก สะท้อนถึงความหมายที่ไม่เกี่ยวข้องกับกลุ่มคำก่อนหน้า

💡 **Note:**  
ตัวอย่างข้างต้นใช้ embedding แบบ 3 มิติ เพื่อความเข้าใจง่าย  แต่ในโมเดลภาษาแบบจริง (เช่น GPT, BERT) embeddings มักมี **หลายร้อยถึงหลายพันมิติ** เพื่อจับความหมายได้ละเอียดลึกขึ้น

  

มีหลายเทคนิคในการสร้าง embedding ที่เหมาะสมกับ token ได้แก่:
- **Word2Vec** 
  ใช้การเรียนรู้ร่วมกันของคำในบริบทใกล้เคียงกัน เพื่อจัดตำแหน่งเวกเตอร์
- **Transformer Encoder Block**  
  ใช้ *attention mechanism* เพื่อสร้าง contextual embedding โดยคำนึงถึงความสัมพันธ์ของคำทั้งหมดในประโยค

### Attention

ทั้ง encoder และ decoder ของโมเดล transformer ประกอบด้วยหลายเลเยอร์ โดยหนึ่งในเลเยอร์สำคัญคือ *attention layer* 

**Attention** คือเทคนิคที่ใช้วิเคราะห์ความสัมพันธ์ระหว่าง *token* ต่าง ๆ ภายในข้อความ เพื่อประเมินว่า token ใดมีผลต่ออีก token หนึ่งมากน้อยเพียงใด โดยเฉพาะ *self-attention* จะพิจารณาว่า token รอบ ๆ มีอิทธิพลต่อความหมายของ token ปัจจุบันอย่างไร

ใน encoder แต่ละ token จะถูกพิจารณาในบริบทของ token อื่น ๆ เพื่อสร้าง *vector embedding* ที่เหมาะสม embedding ที่ได้จึงขึ้นกับบริบท เช่น  

> "the bark of a tree" ≠ "I heard a dog bark"

แม้จะเป็นคำว่า *bark* เหมือนกัน แต่ความหมายและ embedding ต่างกัน

decoder ใช้ attention เพื่อคาดเดา token ถัดไป โดยดู sequence ที่ถูก generate มาก่อนหน้า  
attention จะเน้น token ที่มีอิทธิพลสูง เช่น  

> “I heard a dog” → model จะให้ความสำคัญกับ “heard” และ “dog” มากกว่าเพื่อทำนายว่า token ถัดไปคือ *“bark”*


สิ่งสำคัญที่ต้องเข้าใจคือ *attention layer ไม่ได้ทำงานกับข้อความจริง* แต่ทำงานกับ *vector ตัวเลข* ที่แทนแต่ละ token ใน Decoder: กระบวนการเริ่มต้นด้วย *token embeddings* ที่แทนข้อความที่ต้องการให้ model ทำต่อ  
จากนั้นมี *positional encoding* ถูกเพิ่มเข้าไปใน embedding ของแต่ละ token เพื่อบอกตำแหน่งในลำดับ

```text
- [1,5,6,2] (I)
- [2,9,3,1] (heard)
- [3,1,1,2] (a)
- [4,10,3,2] (dog)
```

เวกเตอร์เหล่านี้รวมทั้ง *ความหมาย* ของคำ และ *ตำแหน่ง* ของคำในประโยค เพื่อให้โมเดลสามารถใช้ attention พิจารณาว่า token ใดควรมีอิทธิพลต่อการทำนาย token ถัดไป

ระหว่างการฝึกโมเดล เป้าหมายคือการทำนายเวกเตอร์ของ token สุดท้ายจากลำดับ token ที่มาก่อนหน้า  
เลเยอร์ attention จะให้ค่าน้ำหนักกับแต่ละ token เพื่อใช้ค่านี้คำนวณเวกเตอร์แบบถ่วงน้ำหนัก และได้ attention score ออกมา ค่าดังกล่าวใช้ในการสร้างเวกเตอร์ของ token ถัดไป

เทคนิคที่เรียกว่า *multi-head attention* จะใช้หลายชุดของ embedding เพื่อคำนวณหลายชุดของ attention score พร้อมกัน  จากนั้นจะใช้ neural network เพื่อประเมิน token ทั้งหมด และเลือก token ที่มีความน่าจะเป็นสูงที่สุดที่จะมาต่อในลำดับ 
กระบวนการนี้ทำซ้ำทีละ token โดยใช้ output ที่สร้างไว้แล้วมาเป็น input สำหรับรอบถัดไป เท่ากับว่าโมเดลสร้างข้อความออกมาทีละ token แบบวนซ้ำต่อเนื่องไปเรื่อยๆ

แอนิเมชันด้านล่างแสดงภาพจำลองแบบง่ายของกระบวนการนี้ – แม้ว่าในความเป็นจริง การคำนวณภายในเลเยอร์ attention จะซับซ้อนกว่านี้  แต่หลักการสามารถอธิบายแบบง่ายได้ตามที่แสดงไว้

![Animation showing an attention layer assigning weights to tokens and predicting the next one.](https://learn.microsoft.com/en-us/training/wwl-data-ai/fundamentals-machine-learning/media/attention.gif)

1. ลำดับของ token embeddings จะถูกป้อนเข้าไปในเลเยอร์ attention โดยแต่ละ token จะแสดงเป็นเวกเตอร์ที่ประกอบด้วยค่าตัวเลข  
2. เป้าหมายของ decoder คือการทำนาย token ถัดไปในลำดับ ซึ่งเป็นเวกเตอร์ที่ต้องสอดคล้องกับ embedding ที่มีอยู่ใน vocabulary ของโมเดล  
3. เลเยอร์ attention จะประเมินลำดับ token ที่มีอยู่ และให้ค่าน้ำหนักกับแต่ละ token เพื่อแสดงระดับอิทธิพลของแต่ละ token ที่มีต่อ token ถัดไป  
4. ค่าน้ำหนักเหล่านี้จะถูกนำไปใช้ในการคำนวณเวกเตอร์ใหม่สำหรับ token ถัดไป โดยใช้ attention score *Multi-head attention* จะใช้หลายส่วนของ embedding เพื่อคำนวณตัวเลือกของ token ที่เป็นไปได้หลายชุด  
5. จากนั้น *fully connected neural network* จะใช้เวกเตอร์เหล่านั้น เพื่อประเมินว่า token ใดใน vocabulary มีความน่าจะเป็นมากที่สุด  
6. token ที่ถูกทำนายจะถูกเพิ่มเข้าไปในลำดับ และนำลำดับใหม่นี้กลับมาใช้เป็น input ในรอบถัดไป

ระหว่างการฝึก โมเดลจะรู้ลำดับของ token ทั้งหมด แต่จะ *mask* token ที่อยู่ถัดจากตำแหน่งปัจจุบัน  
เพื่อไม่ให้โมเดลเห็นคำในอนาคต  โมเดลจะทำนายเวกเตอร์ของ token ถัดไป แล้วนำไปเทียบกับเวกเตอร์จริง เพื่อคำนวณ *loss* จากนั้นค่อย ๆ ปรับ *weight* เพื่อให้ผลลัพธ์แม่นยำขึ้นตามหลักของ neural network 

เมื่อใช้งานจริง (*inferencing*)  โมเดลจะใช้ *attention layer* ที่ฝึกไว้  เพื่อเลือก token ที่มีแนวโน้มจะตรงกับความหมายของลำดับข้อความก่อนหน้า  โดยเลือกจากคำในพจนานุกรมทั้งหมดของโมเดล

โมเดลแบบ transformer เช่น GPT-4 (ซึ่งเป็นเบื้องหลังของ ChatGPT และ Bing) ถูกออกแบบมาเพื่อรับข้อความที่เรียกว่า _prompt_ แล้วสร้างข้อความตอบกลับที่ถูกต้องตามหลักไวยากรณ์ (_completion_) ออกมา จุดเด่นของมันคือความสามารถในการเรียงประโยคให้ต่อเนื่องและมีความหมาย แม้ความสามารถนี้จะดูคล้าย “ความรู้” หรือ “ความฉลาด” แต่จริง ๆ แล้วโมเดลแค่มีคลังคำขนาดใหญ่และสามารถเรียงคำออกมาเป็นประโยคที่ดูมีความหมายเท่านั้น
 
สิ่งที่ทำให้ GPT-4 ทรงพลังคือ:
- ปริมาณข้อมูลที่ใช้ในการฝึกมีมหาศาล (รวมถึงข้อมูลสาธารณะและที่มีลิขสิทธิ์จากอินเทอร์เน็ต)
- โครงข่ายของโมเดลมีความซับซ้อนสูง  

ทั้งหมดนี้ทำให้โมเดลสามารถสร้างข้อความตอบกลับได้โดยอิงจากความสัมพันธ์ระหว่างคำต่าง ๆ ที่มันเคยฝึกมา และบ่อยครั้งผลลัพธ์ที่ได้จะดูแทบไม่ต่างจากคำตอบของมนุษย์ในสถานการณ์เดียวกัน
