
เมื่อ _NLP_ (Natural Language Processing) พัฒนาก้าวหน้าไปเรื่อย ๆ ความสามารถในการฝึกโมเดลให้เข้าใจ _ความสัมพันธ์เชิงความหมาย_ (_semantic relationship_) ระหว่างคำต่าง ๆ ก็เพิ่มขึ้นตามไปด้วย ซึ่งนำไปสู่การเกิดขึ้นของโมเดลภาษาที่ใช้ _deep learning_ ซึ่งมีความสามารถสูงมาก

หัวใจสำคัญของโมเดลเหล่านี้คือการเข้ารหัสคำ (หรือ token) ให้อยู่ในรูปแบบของเวกเตอร์ (_vector_) ซึ่งเป็นอาเรย์ของตัวเลขหลายค่า ที่เราเรียกว่า _embeddings_

เวกเตอร์เปรียบเสมือนเส้นตรงใน _พื้นที่หลายมิติ_ ซึ่งมีทั้ง _ทิศทาง_ และ _ระยะทาง_ โดยรวมแล้ว เวกเตอร์จะแสดงทิศทางและระยะจากจุดเริ่มต้น (origin) ไปยังจุดปลาย

หากคำใด ๆ มีความหมายใกล้เคียงกัน คำเหล่านั้นควรถูกเข้ารหัสออกมาเป็นเวกเตอร์ที่มีทิศทางใกล้เคียงกัน หรือกล่าวอีกอย่างคือ _ชี้ไปทางเดียวกัน_

ยกตัวอย่างง่าย ๆ สมมุติว่า embedding ของแต่ละคำเป็นเวกเตอร์ที่มี 3 ค่า เช่น:

```text
- 4 ("dog"): [10,3,2]
- 5 ("bark"): [10,2,2]
- 8 ("cat"): [10,3,1]
- 9 ("meow"): [10,2,1]
- 10 ("skateboard"): [-3,3,2]
```

ใน _พื้นที่สามมิติ_ (_three-dimensional space_) เวกเตอร์เหล่านี้จะมีหน้าตาประมาณนี้:

[[04-03 How do language models work?]]

![A diagram of tokens plotted on a three-dimensional space.](https://learn.microsoft.com/en-us/training/wwl-data-ai/introduction-language/media/word-embeddings.png)


เวกเตอร์ _embedding_ สำหรับคำว่า _"dog"_ และ _"puppy"_ แสดงเส้นทางในทิศทางที่เกือบเหมือนกัน และยังมีทิศทางที่คล้ายกับคำว่า _"cat"_ อีกด้วย นั่นหมายความว่าโมเดลเข้าใจว่าคำเหล่านี้มี _ความหมายใกล้เคียงกัน_

แต่สำหรับคำว่า _"skateboard"_ เวกเตอร์จะชี้ไปในทิศทางที่แตกต่างออกไปมาก แสดงว่า _ความหมายของคำนี้แตกต่างจากคำกลุ่มแรกอย่างชัดเจน_

โมเดลภาษาที่ใช้ในภาคอุตสาหกรรมทุกวันนี้ก็ใช้หลักการเดียวกันนี้ แต่มีความซับซ้อนมากกว่า ตัวอย่างเช่น เวกเตอร์ที่ใช้อาจมี _หลายร้อยหรือหลายพันมิติ_ (ไม่ใช่แค่สามมิติ)

นอกจากนี้ ยังมีวิธีการหลายแบบในการคำนวณ _embedding_ ที่เหมาะสมสำหรับชุดของ _tokens_ ซึ่งแต่ละวิธีก็อาจให้ผลลัพธ์ที่แตกต่างกันในการทำนายของโมเดล _natural language processing_ เช่นกัน


ภาพรวมทั่วไปของโซลูชัน _natural language processing_ สมัยใหม่ส่วนใหญ่สามารถอธิบายได้ตามแผนภาพด้านล่างนี้:

ระบบจะเริ่มต้นด้วย _ข้อมูลข้อความขนาดใหญ่_ (_large corpus of raw text_) ที่ยังไม่ได้ประมวลผล จากนั้นข้อความจะถูกแปลงเป็น _tokens_ ผ่านกระบวนการที่เรียกว่า _tokenization_ เมื่อได้ _tokens_ แล้ว ข้อมูลเหล่านี้จะถูกนำไปใช้ฝึก _language model_ ซึ่งเป็นโมเดลที่เรียนรู้ _โครงสร้าง_ และ _ความหมาย_ ของภาษา โมเดลภาษาเหล่านี้สามารถนำไปใช้กับงานต่าง ๆ ในด้าน _natural language processing_ ได้หลากหลาย เช่น การ _แปลภาษา_, _วิเคราะห์ความรู้สึก_, _สรุปข้อความ_, หรือ _ตอบคำถาม_

การใช้โมเดลเดียวกันรองรับหลายงานได้ เป็นผลจากความสามารถของโมเดลในการเข้าใจ _บริบท_ และ _ความสัมพันธ์เชิงความหมาย_ ของคำในระดับลึก

![A diagram of the process to tokenize text and train a language model that supports natural language processing tasks.](https://learn.microsoft.com/en-us/training/wwl-data-ai/introduction-language/media/language-model.png)

## Machine learning for text classification

อีกเทคนิคหนึ่งที่มีประโยชน์ในการวิเคราะห์ข้อความคือการใช้ _classification algorithm_ เช่น _logistic regression_ เพื่อฝึกโมเดล _machine learning_ ที่สามารถจัดประเภทข้อความตามกลุ่มที่กำหนดไว้ล่วงหน้า

ตัวอย่างที่พบบ่อยคือการฝึกโมเดลให้จำแนกข้อความว่าเป็น _แง่บวก_ หรือ _แง่ลบ_ เพื่อใช้ในงาน _sentiment analysis_ หรือ _opinion mining_

เช่น ลองพิจารณารีวิวร้านอาหารต่อไปนี้ ซึ่งมีการติดป้ายกำกับไว้แล้วว่าเป็น 0 (_negative_) หรือ 1 (_positive_):

```text
- *The food and service were both great*: 1
- *A really terrible experience*: 0
- *Mmm! tasty food and a fun vibe*: 1
- *Slow service and substandard food*: 0
```

เมื่อมีรีวิวที่ถูกติดป้ายกำกับ (_labeled_) เพียงพอ คุณสามารถฝึกโมเดล _classification_ ได้โดยใช้ข้อความที่ผ่านการ _tokenize_ แล้วเป็น _features_ และใช้ค่า _sentiment_ (0 หรือ 1) เป็น _label_

โมเดลที่ได้จะเรียนรู้ _ความสัมพันธ์ระหว่าง tokens กับอารมณ์ของข้อความ_ เช่น ถ้ารีวิวมีคำว่า _"great"_, _"tasty"_ หรือ _"fun"_ ก็มีแนวโน้มว่าจะได้รับค่าผลลัพธ์เป็น 1 (_positive_)

ในทางกลับกัน ถ้ารีวิวมีคำว่า _"terrible"_, _"slow"_ หรือ _"substandard"_ ก็มีแนวโน้มว่าจะถูกจัดให้อยู่ในกลุ่มที่มีค่าเป็น 0 (_negative_)

วิธีนี้ทำให้โมเดลสามารถทำนาย _sentiment_ ของข้อความใหม่ได้ แม้จะไม่เคยเห็นข้อความนั้นมาก่อน

