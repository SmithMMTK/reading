
เมื่อคุณได้ _ระบุความเสี่ยง_ (_map potential harms_) พัฒนาแนวทางในการ _วัดผล_ และ _ดำเนินการลดความเสี่ยง_ ในโซลูชันเรียบร้อยแล้ว ขั้นตอนถัดไปคือการเตรียมความพร้อมสำหรับการ _เผยแพร่_ โซลูชัน

แต่ก่อนที่จะปล่อยใช้งานจริง ยังมีประเด็นสำคัญบางอย่างที่ควรพิจารณาเพิ่มเติม เพื่อให้แน่ใจว่าโซลูชันของคุณจะถูกนำไปใช้อย่าง _ปลอดภัย_ และ _มีประสิทธิภาพ_ ทั้งในช่วงเปิดตัวและระหว่างการใช้งานในระยะยาว

## Complete prerelease reviews

ก่อนที่คุณจะเผยแพร่โซลูชัน _generative AI_ ควรตรวจสอบว่าได้ปฏิบัติตาม _ข้อกำหนดด้านการปฏิบัติตามกฎระเบียบ_ (_compliance requirements_) ที่เกี่ยวข้องกับ _องค์กร_ และ _อุตสาหกรรม_ ของคุณหรือไม่ และต้องให้ทีมที่เกี่ยวข้องมีโอกาส _ตรวจสอบระบบและเอกสาร_ ประกอบก่อนปล่อยใช้งาน การตรวจสอบด้าน compliance ที่พบบ่อย ได้แก่:

- Legal
- Privacy
- Security
- Accessibility

## Release and operate the solution

การเผยแพร่โซลูชันให้ประสบความสำเร็จจำเป็นต้องมี _การวางแผน_ และ _การเตรียมความพร้อม_ ล่วงหน้า โดยควรพิจารณาแนวทางต่อไปนี้:

- วางแผน _phased delivery_ โดยเริ่มปล่อยให้เฉพาะกลุ่มผู้ใช้งานที่จำกัดก่อน เพื่อ _เก็บ feedback_ และระบุปัญหาก่อนเปิดให้ใช้งานในวงกว้าง
- สร้าง _incident response plan_ ที่กำหนดขั้นตอนการตอบสนองเมื่อเกิดเหตุการณ์ไม่คาดคิด พร้อมทั้งระบุ _เวลาที่คาดว่าจะใช้ในการตอบสนอง_
- สร้าง _rollback plan_ ที่กำหนดขั้นตอนในการย้อนกลับระบบไปยังสถานะก่อนหน้า หากเกิดเหตุการณ์ที่ต้องหยุดระบบหรือย้อนการเปลี่ยนแปลง
- มีความสามารถในการ _บล็อก output ที่เป็นอันตรายได้ทันที_ เมื่อมีการค้นพบ
- มีระบบที่สามารถ _บล็อกผู้ใช้ แอปพลิเคชัน หรือ IP address_ เฉพาะได้ในกรณีที่พบการใช้งานผิดวัตถุประสงค์
- ให้ผู้ใช้สามารถ _รายงาน feedback_ และ _แจ้งปัญหา_ โดยเฉพาะอย่างยิ่งการรายงานเนื้อหาที่ถูกสร้างว่าเป็น “_inaccurate_”, “_incomplete_”, “_harmful_”, “_offensive_” หรือ _ปัญหาอื่น ๆ_
- เก็บข้อมูล _telemetry_ เพื่อตรวจสอบความพึงพอใจของผู้ใช้ และระบุช่องว่างในฟังก์ชันหรือปัญหาด้านการใช้งาน โดยการเก็บข้อมูลนี้ต้องเป็นไปตาม _กฎหมายความเป็นส่วนตัว_ และ _นโยบายขององค์กร_

แนวทางเหล่านี้จะช่วยให้การเปิดตัวโซลูชัน _ปลอดภัย_ _ควบคุมได้_ และ _พร้อมรับมือกับเหตุการณ์ที่ไม่คาดคิด_

## Utilize Azure AI Foundry Content Safety

Azure AI หลายบริการมีความสามารถในการ _วิเคราะห์เนื้อหาในตัว_ (_built-in content analysis_) เช่น _Language_, _Vision_, และ _Azure OpenAI_ ซึ่งใช้ _content filters_ เพื่อช่วยระบุและจัดการเนื้อหาที่อาจไม่เหมาะสม

สำหรับการควบคุมความปลอดภัยในระดับสูงขึ้น _Azure AI Foundry Content Safety_ มีฟีเจอร์เฉพาะที่เน้นปกป้อง _AI และ copilots_ จากความเสี่ยงต่าง ๆ โดยสามารถตรวจจับภาษาไม่เหมาะสมหรือรุนแรงได้ ทั้งจาก _input ที่ผู้ใช้ป้อน_ และ _output ที่ระบบสร้าง_

ฟีเจอร์สำคัญใน _Foundry Content Safety_ ได้แก่:

| _Feature_                    | _Functionality_                                                                 |
|-----------------------------|----------------------------------------------------------------------------------|
| _Prompt shields_            | ตรวจสอบความเสี่ยงจากการโจมตีระบบด้วย _user input_ ที่พุ่งเป้าโจมตีโมเดลภาษาโดยตรง   |
| _Groundedness detection_    | ตรวจสอบว่า _ข้อความที่ตอบกลับ_ มี _การอ้างอิงจากข้อมูลต้นทางของผู้ใช้_ หรือไม่           |
| _Protected material detection_ | ตรวจหา _เนื้อหาที่มีลิขสิทธิ์_ ซึ่งอาจถูกนำมาใช้อย่างไม่เหมาะสม                              |
| _Custom categories_         | สร้างหมวดหมู่เฉพาะเพื่อรองรับ _รูปแบบความเสี่ยงใหม่หรือเฉพาะทาง_ ที่อาจเกิดขึ้นในอนาคต     |

คุณสามารถดูรายละเอียดและเริ่มต้นใช้งาน _Foundry Content Safety_ ได้จากหน้าเอกสารอย่างเป็นทางการที่นี่  
[**documentation pages**](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview)

