
เมื่อคุณกำหนด _baseline_ และวิธีการวัด _harmful output_ ได้แล้ว ขั้นตอนถัดไปคือการดำเนินการเพื่อลดความเสี่ยง (_mitigation_) ที่อาจเกิดขึ้น และเมื่อปรับปรุงระบบแล้ว ควรทำการ _ทดสอบซ้ำ_ เพื่อเปรียบเทียบระดับความเสี่ยงกับ baseline ที่ตั้งไว้

การลดความเสี่ยงในโซลูชัน _generative AI_ ใช้แนวทางแบบ _หลายชั้น_ (_layered approach_) ซึ่งสามารถนำเทคนิคการลดความเสี่ยงไปประยุกต์ใช้ได้ในแต่ละชั้น ทั้งหมด 4 ชั้น ดังนี้:

![Diagram showing the model, safety system, application, and positioning layers of a generative AI solution.](https://learn.microsoft.com/en-us/training/wwl-data-ai/responsible-ai-studio/media/mitigate-harms.png)

1. **Model**
2. **Safety System**
3. **System message and grounding**
4. **User experience**

## 1: The _model_ layer

ชั้นที่หนึ่งคือ _model layer_ ซึ่งหมายถึงโมเดล _generative AI_ หนึ่งตัวหรือหลายตัวที่เป็นหัวใจหลักของโซลูชันคุณ เช่น โซลูชันอาจสร้างขึ้นจากโมเดลอย่าง _GPT-4_

แนวทางการลดความเสี่ยงที่สามารถนำมาใช้ใน _model layer_ ได้แก่:

- การเลือกใช้ _โมเดลที่เหมาะสม_ กับการใช้งาน เช่น แม้ว่า GPT-4 จะเป็นโมเดลที่ทรงพลังและมีความสามารถหลากหลาย แต่หากโซลูชันของคุณมีแค่การ _จำแนกข้อความสั้น ๆ_ แบบเฉพาะเจาะจง การใช้โมเดลที่เรียบง่ายกว่าอาจเพียงพอและมี _ความเสี่ยงต่ำกว่า_ ในการสร้างเนื้อหาที่เป็นอันตราย

- การ _fine-tune_ โมเดลพื้นฐาน (_foundational model_) ด้วยข้อมูลการฝึกอบรมของคุณเอง เพื่อให้ _คำตอบที่สร้างขึ้นมีความเกี่ยวข้อง_ และ _อยู่ในขอบเขต_ ที่เหมาะกับบริบทของโซลูชันนั้น ๆ

## 2: The _safety system_ layer

ชั้นที่สองคือ _safety system layer_ ซึ่งหมายถึงการตั้งค่าและความสามารถในระดับแพลตฟอร์มที่ช่วย _ลดความเสี่ยง_ จากเนื้อหาที่เป็นอันตราย

ตัวอย่างเช่น _Azure AI Foundry_ มีระบบ _content filters_ ที่สามารถตั้งเกณฑ์เพื่อ _ระงับ prompt หรือคำตอบ_ โดยใช้การจัดระดับความรุนแรงออกเป็น 4 ระดับ (_safe_, _low_, _medium_, _high_) สำหรับ 4 หมวดหมู่ความเสี่ยง ได้แก่

- _hate_ (ความเกลียดชัง)  
- _sexual_ (เนื้อหาเกี่ยวกับเพศ)  
- _violence_ (ความรุนแรง)  
- _self-harm_ (การทำร้ายตัวเอง)

แนวทางการลดความเสี่ยงใน _safety system layer_ อื่น ๆ อาจรวมถึง:

- _abuse detection algorithm_ ที่ช่วยตรวจจับว่าโซลูชันกำลังถูกใช้ในทางที่ผิดหรือไม่ เช่น การมี request จำนวนมากผิดปกติจาก bot  
- _alert notifications_ ที่แจ้งเตือนเมื่อมีความเสี่ยงเกิดขึ้น เพื่อให้ทีมสามารถ _ตอบสนองได้อย่างรวดเร็ว_ ต่อพฤติกรรมที่อันตรายหรือการใช้ระบบผิดวิธี

## 3: The _system message and grounding_ layer

ชั้นที่สามคือ _prompt layer_ ซึ่งเน้นที่การออกแบบและจัดการ _prompt_ ที่จะถูกส่งเข้าไปยังโมเดล

เทคนิคการลดความเสี่ยงที่สามารถนำมาใช้ในชั้นนี้ ได้แก่:

- การกำหนด _system inputs_ ที่ระบุพฤติกรรมหรือขอบเขตการทำงานของโมเดล เช่น บอกให้โมเดลหลีกเลี่ยงการแสดงความคิดเห็นเกี่ยวกับประเด็นอ่อนไหว
- การใช้ _prompt engineering_ โดยเติมข้อมูล _grounding_ ลงไปใน prompt เพื่อเพิ่มโอกาสให้โมเดลตอบในลักษณะที่ _เกี่ยวข้องและไม่ก่อให้เกิดอันตราย_
- การใช้แนวทาง _retrieval augmented generation_ (_RAG_) เพื่อดึงข้อมูลจากแหล่งข้อมูลที่ _เชื่อถือได้_ มาใช้ประกอบใน prompt ซึ่งช่วยให้ผลลัพธ์ _ตรงประเด็น_ และลดความเสี่ยงจากข้อมูลผิดพลาด

## 4: The _user experience_ layer

ชั้นสุดท้ายคือ _user experience layer_ ซึ่งครอบคลุมทั้ง _ซอฟต์แวร์หรือแอปพลิเคชัน_ ที่ผู้ใช้ใช้ในการโต้ตอบกับโมเดล generative AI และ _เอกสารหรือคู่มือ_ ที่อธิบายการใช้งานระบบแก่ผู้ใช้และผู้มีส่วนเกี่ยวข้อง

แนวทางการลดความเสี่ยงในชั้นนี้ ได้แก่:

- ออกแบบ _user interface_ ของแอปพลิเคชันให้ _จำกัดประเภทของ input_ เช่น ให้เลือกหัวข้อจากรายการแทนการพิมพ์อิสระ หรือใช้การ _ตรวจสอบ input และ output_ เพื่อช่วยลดโอกาสที่โมเดลจะสร้างเนื้อหาที่เป็นอันตราย

- จัดทำ _เอกสารและคำอธิบายระบบ_ ที่มีความ _โปร่งใส_ ระบุอย่างชัดเจนว่าโซลูชันมีขีดความสามารถและข้อจำกัดอะไร ใช้โมเดลใดเป็นพื้นฐาน และยังมี _ความเสี่ยงใดที่อาจยังไม่สามารถจัดการได้_ แม้มีมาตรการลดความเสี่ยงในระดับต่าง ๆ แล้วก็ตาม