
ส่วนสำคัญอย่างมากของ _machine learning_ คือการประเมินว่าโมเดลทำงานได้ดีแค่ไหน การประเมินนี้เกิดขึ้นทั้งในช่วง _training_ เพื่อช่วยปรับปรุงโมเดล และหลังจาก _training_ เสร็จแล้ว เพื่อให้เราตัดสินใจได้ว่าโมเดลนั้นเหมาะสมสำหรับใช้งานจริงหรือไม่

โมเดล _classification_ ก็ต้องได้รับการประเมินเช่นเดียวกับ _regression_ แต่กระบวนการประเมินของ _classification_ อาจซับซ้อนกว่าเล็กน้อยในบางกรณี

## A refresher on cost

อย่าลืมว่าในระหว่าง _training_ เราจะคำนวณว่าโมเดลทำงานได้แย่แค่ไหน ซึ่งเราเรียกค่านี้ว่า _cost_ หรือ _loss_ ตัวอย่างเช่น ใน _linear regression_ เรามักใช้ _performance metric_ ที่เรียกว่า _mean-squared error (MSE)_

_MSE_ คำนวณโดยการเปรียบเทียบค่าที่โมเดลทำนายกับค่าจริง (_actual label_) หาค่าความต่าง ยกกำลังสอง แล้วนำค่าเฉลี่ยของผลลัพธ์เหล่านั้นมาใช้ ค่านี้สามารถใช้ทั้งในการปรับแต่งโมเดล (_fit_) และใช้รายงานว่าโมเดลทำงานได้ดีแค่ไหน

## Cost functions for classification

การประเมินโมเดล _classification_ สามารถทำได้สองแบบ คือดูจากค่าความน่าจะเป็นที่โมเดลให้มา (เช่น มีโอกาสเกิดหิมะถล่ม 40%) หรือดูจาก _final labels_ เช่น `no avalanche` หรือ `avalanche`

การใช้ _output probabilities_ จะมีข้อดีในช่วง _training_ เพราะแม้โมเดลจะเปลี่ยนเพียงเล็กน้อย ก็สามารถเห็นการเปลี่ยนแปลงใน _probability_ ได้ทันที แม้ผลลัพธ์สุดท้ายจะยังไม่เปลี่ยนก็ตาม

แต่ถ้าเราต้องการประเมิน _performance_ ของโมเดลในโลกจริง โดยเฉพาะเมื่อทดสอบกับ _test set_ การใช้ _final labels_ เพื่อคำนวณ _cost function_ จะมีประโยชน์มากกว่า เพราะในสถานการณ์จริง เราจะใช้ผลลัพธ์สุดท้าย ไม่ได้ใช้แค่ความน่าจะเป็น

## Log loss

_log loss_ เป็นหนึ่งใน _cost functions_ ที่นิยมใช้สำหรับงาน _classification_ แบบง่าย โดยจะใช้กับ _output probabilities_ คล้ายกับ _MSE_ ตรงที่ข้อผิดพลาดน้อยจะให้ค่า _cost_ ต่ำ และถ้าผิดพลาดมากก็จะให้ _cost_ สูงขึ้นตาม

เราสามารถวาดกราฟของ _log loss_ ได้ เช่นในกรณีที่ _label_ ที่ถูกต้องคือ 0 (_false_) โดยจะเห็นว่าเมื่อโมเดลทำนายเข้าใกล้ 0 ค่า _loss_ จะต่ำ แต่ถ้ายิ่งทำนายไปใกล้ 1 (ผิดมาก) _loss_ ก็จะพุ่งสูงขึ้นอย่างรวดเร็ว

![Diagram showing a log loss example graph.](https://learn.microsoft.com/en-us/training/modules/understand-classification-machine-learning/media/4-log-loss-graph.png)

แกน x ในกราฟแสดงผลลัพธ์ที่เป็นไปได้ของโมเดล ซึ่งก็คือค่าความน่าจะเป็นตั้งแต่ 0 ถึง 1 ส่วนแกน y แสดงค่า _cost_ หรือ _loss_

ถ้าโมเดลมั่นใจว่าคำตอบควรเป็น 0 (เช่น ทำนายออกมาเป็น 0.1) _cost_ จะต่ำ เพราะในกรณีนี้คำตอบที่ถูกต้องคือ 0 แต่ถ้าโมเดลทำนายผิดอย่างมั่นใจ (เช่น ทำนายออกมาเป็น 0.9 ทั้งที่จริงควรเป็น 0) _cost_ จะสูงมาก

ในความเป็นจริง ถ้าโมเดลทำนายเป็น 1 ค่า _cost_ จะสูงมากจนเราไม่สามารถแสดงบนกราฟได้ทั้งหมด ดังนั้นในกราฟจึงตัดแกน x ไว้ที่ 0.999 เพื่อให้ยังอ่านค่าได้สะดวก

## Why not MSE?

_MSE_ และ _log loss_ เป็น _performance metrics_ ที่คล้ายกัน แต่มีเหตุผลบางอย่างที่ทำให้ _log loss_ ได้รับความนิยมมากกว่าในการใช้งานกับ _logistic regression_ ซึ่งบางเหตุผลก็ซับซ้อนทางคณิตศาสตร์ แต่ก็มีเหตุผลง่าย ๆ ด้วยเช่นกัน

เหตุผลหนึ่งคือ _log loss_ จะลงโทษคำตอบที่ผิดอย่างรุนแรงกว่ามากเมื่อเทียบกับ _MSE_ ตัวอย่างเช่น ถ้าคำตอบที่ถูกต้องคือ 0 แล้วโมเดลทำนายค่าออกมาเกิน 0.8 ในกราฟจะเห็นว่า _log loss_ ให้ค่า _cost_ สูงกว่า _MSE_ อย่างชัดเจน

![Diagram showing a log loss versus mse graph.](https://learn.microsoft.com/en-us/training/modules/understand-classification-machine-learning/media/4-mse-graph.png)

การที่ _log loss_ ให้ค่า _cost_ สูงกว่าในกรณีที่โมเดลทำนายผิดมากนั้น ช่วยให้โมเดล _เรียนรู้ได้เร็วขึ้น_ เพราะเส้นกราฟมีความชันมาก (_steeper gradient_) ซึ่งส่งผลให้การปรับค่าพารามิเตอร์ในแต่ละรอบของ _training_ มีประสิทธิภาพมากขึ้น

อีกทั้ง _log loss_ ยังช่วยให้โมเดลมีความมั่นใจในการให้คำตอบที่ถูกต้องมากขึ้น

ลองสังเกตจากกราฟก่อนหน้านี้ จะเห็นว่าเมื่อค่าทำนายน้อยกว่า 0.2 ค่า _MSE_ จะต่ำมากและเส้นกราฟแบนราบ นั่นหมายความว่าโมเดลจะเรียนรู้ได้ช้ามากแม้จะใกล้ถูกแล้ว ในทางกลับกัน _log loss_ ยังคงมีความชันที่สูง ทำให้โมเดลสามารถเรียนรู้ต่อได้เร็วกว่าในช่วงที่ค่าทำนายใกล้ถูกอยู่แล้ว

## Limitations of cost functions

การใช้ _cost function_ เพียงค่าเดียวในการประเมินโมเดลอาจมีข้อจำกัด เพราะมันไม่สามารถบอกได้ว่าโมเดลกำลังทำผิดแบบไหน

ตัวอย่างเช่น ในกรณีการทำนายหิมะถล่ม ค่า _log loss_ ที่สูงอาจหมายถึงว่าโมเดลทำนายว่ามีหิมะถล่มบ่อยเกินไป ทั้งที่ไม่มีจริง หรืออาจหมายถึงว่าโมเดลพลาดในการทำนายหิมะถล่มที่เกิดขึ้นจริงอย่างต่อเนื่อง

เพื่อให้เข้าใจโมเดลได้ดีขึ้น เราจึงควรใช้มากกว่าหนึ่งค่าในการประเมินว่าโมเดลทำงานดีแค่ไหน ซึ่งหัวข้อนี้จะถูกอธิบายเพิ่มเติมในบทเรียนอื่น ๆ ต่อไป และเราจะได้เริ่มสัมผัสบางส่วนในแบบฝึกหัดถัดจากนี้

แนวทางเหล่านี้เกี่ยวข้องกับ _performance metrics_ ที่หลากหลาย ซึ่งช่วยให้เราแยกแยะได้ว่าโมเดลกำลังทำพลาดในรูปแบบไหน เช่น _precision_, _recall_ และ _confusion matrix_