
_Dataflows Gen2_ เป็นทางเลือกที่ยอดเยี่ยมสำหรับการแปลงข้อมูลใน _Microsoft Fabric_ และเมื่อใช้งานร่วมกับ _data pipelines_ ก็จะมีประโยชน์มากในกรณีที่คุณต้องการดำเนินการเพิ่มเติมกับข้อมูลที่ถูกแปลงแล้ว

_Data pipelines_ เป็นแนวคิดทั่วไปในงาน _data engineering_ ซึ่งมี _activities_ ให้เลือกใช้หลากหลายเพื่อกำหนดลำดับการทำงาน ตัวอย่างกิจกรรมทั่วไป ได้แก่:

- คัดลอกข้อมูล (_Copy data_)
- เพิ่ม _Dataflow_
- เพิ่ม _Notebook_
- ดึงข้อมูลเมตา (_Get metadata_)
- รัน _script_ หรือ _stored procedure_

_Pipelines_ มีอินเทอร์เฟซแบบภาพให้คุณจัดลำดับการทำงานตามขั้นตอนที่ต้องการได้อย่างชัดเจน คุณสามารถใช้ _dataflow_ เพื่อดึง แปลง และโหลดข้อมูลเข้าสู่ _Fabric data store_ แล้วจึงเพิ่ม _dataflow_ นั้นเข้าไปใน _pipeline_ เพื่อสั่งรันกิจกรรมเพิ่มเติม เช่น การรัน _script_ หรือ _stored procedure_ หลังจาก _dataflow_ ทำงานเสร็จสิ้น

_Pipelines_ ยังสามารถตั้งเวลาให้รัน หรือให้รันโดยอัตโนมัติผ่าน _trigger_ เพื่อควบคุมการทำงานของ _dataflow_ ได้ การใช้ _pipeline_ ในการรัน _dataflow_ ช่วยให้คุณสามารถรีเฟรชข้อมูลได้ตามเวลาที่กำหนด โดยไม่ต้องสั่งรันด้วยตนเองทุกครั้ง ซึ่งเป็นประโยชน์อย่างมากเมื่อต้องจัดการข้อมูลระดับองค์กรหรือข้อมูลที่มีการเปลี่ยนแปลงบ่อย ช่วยให้คุณมีเวลาสำหรับหน้าที่อื่น ๆ มากขึ้น

![Screenshot of the pipeline schedule window for a dataflow.](https://learn.microsoft.com/en-us/training/wwl/use-dataflow-gen-2-fabric/media/dataflow-schedule-pipeline.png)