
ในสถานการณ์ของเรา คุณจำเป็นต้องพัฒนา _semantic model_ ที่สามารถทำให้ข้อมูลมีรูปแบบมาตรฐาน และเปิดให้ธุรกิจเข้าถึงข้อมูลได้ โดยการใช้ _Dataflows Gen2_ คุณสามารถเชื่อมต่อกับแหล่งข้อมูลหลากหลาย และดำเนินการเตรียมและแปลงข้อมูลได้ จากนั้นคุณสามารถส่งข้อมูลไปยัง _lakehouse_ โดยตรง หรือใช้ _data pipeline_ เพื่อส่งต่อไปยังปลายทางอื่น

## What is a dataflow?

_Dataflows_ คือเครื่องมือแบบ _cloud-based ETL (Extract, Transform, Load)_ สำหรับการสร้างและประมวลผลกระบวนการแปลงข้อมูลในระดับที่ขยายขนาดได้

_Dataflows Gen2_ ช่วยให้คุณสามารถดึงข้อมูลจากหลายแหล่ง แปลงข้อมูลผ่านชุดของการดำเนินการแปลง และโหลดข้อมูลไปยังปลายทางที่ต้องการ โดยสามารถใช้ _Power Query Online_ เพื่อจัดการงานเหล่านี้ผ่านอินเทอร์เฟซแบบ _visual_

โดยพื้นฐานแล้ว _dataflow_ จะรวบรวมขั้นตอนการแปลงทั้งหมดไว้เพื่อลดเวลาการเตรียมข้อมูล แล้วสามารถโหลดเป็น _table_ ใหม่ ใช้ใน _data pipeline_ หรือเป็นแหล่งข้อมูลให้ _data analyst_ นำไปใช้งานต่อ

## How to use Dataflows Gen2

โดยปกติแล้ว _data engineer_ จะใช้เวลาจำนวนมากในการดึง แปลง และโหลดข้อมูลให้อยู่ในรูปแบบที่พร้อมใช้สำหรับ _analytics_ ขั้นตอนถัดไป _Dataflows Gen2_ มีเป้าหมายเพื่อให้สามารถทำ _ETL_ ได้ง่ายขึ้น นำกลับมาใช้ซ้ำได้ และทำงานผ่าน _Power Query Online_

หากคุณเลือกใช้เฉพาะ _data pipeline_ คุณจะทำการคัดลอกข้อมูล จากนั้นใช้ภาษาโปรแกรมที่คุณถนัดในการดำเนินการ _extract_, _transform_, และ _load_ แต่ในอีกทางหนึ่ง คุณสามารถสร้าง _Dataflow Gen2_ ก่อน เพื่อดึงและแปลงข้อมูล แล้วค่อยโหลดข้อมูลไปยัง _lakehouse_ หรือปลายทางอื่น ทำให้ธุรกิจสามารถใช้ _semantic model_ ที่ผ่านการจัดเตรียมมาแล้วได้อย่างสะดวก

การเพิ่มปลายทางข้อมูลให้กับ _dataflow_ เป็นทางเลือก ไม่ใช่ข้อบังคับ และ _dataflow_ จะคงไว้ซึ่งทุกขั้นตอนการแปลงข้อมูล หากต้องการทำงานอื่น ๆ หรือโหลดข้อมูลไปยังปลายทางอื่นหลังจากการแปลง คุณสามารถสร้าง _data pipeline_ และเพิ่ม _Dataflow Gen2 activity_ เข้ามาในขั้นตอนการจัดลำดับงาน

อีกทางเลือกหนึ่งคือการใช้ _data pipeline_ ร่วมกับ _Dataflow Gen2_ เพื่อดำเนินการแบบ _ELT (Extract, Load, Transform)_ กล่าวคือ ใช้ _pipeline_ เพื่อดึงและโหลดข้อมูลไปยังปลายทาง เช่น _lakehouse_ แล้วค่อยสร้าง _Dataflow Gen2_ เพื่อเชื่อมต่อกับข้อมูลใน _lakehouse_ เพื่อทำความสะอาดและแปลงข้อมูล และในกรณีนี้ คุณสามารถนำเสนอ _Dataflow_ เป็น _semantic model_ สำหรับนักวิเคราะห์ข้อมูลในการสร้างรายงาน

นอกจากนี้ _Dataflows_ ยังสามารถแบ่งข้อมูลในแนวนอนได้ (_horizontal partitioning_) เมื่อคุณสร้าง _global dataflow_ แล้ว _data analysts_ ก็สามารถนำ _dataflow_ นั้นไปสร้าง _semantic model_ เฉพาะทางได้ตามความต้องการ

_Dataflows_ ส่งเสริมให้สามารถแบ่งปันตรรกะ _ETL_ แบบใช้ซ้ำได้ โดยไม่ต้องสร้างการเชื่อมต่อกับแหล่งข้อมูลใหม่ทุกครั้ง _Dataflows_ รองรับการแปลงข้อมูลได้หลากหลายรูปแบบ และสามารถเรียกใช้งานแบบแมนนวล ตามตารางเวลา หรือเป็นส่วนหนึ่งของ _pipeline orchestration_

 **Tip**

ตั้งค่าให้ _dataflow_ ของคุณสามารถค้นหาได้ (_discoverable_) เพื่อให้นักวิเคราะห์ข้อมูลสามารถเชื่อมต่อผ่าน _Power BI Desktop_ ซึ่งช่วยลดภาระในการเตรียมข้อมูลสำหรับการพัฒนารายงาน

## Benefits and limitations

มีหลายวิธีในการทำ _ETL_ หรือ _ELT_ ใน _Microsoft Fabric_ ลองพิจารณาข้อดีและข้อจำกัดของการใช้ _Dataflows Gen2_

**Benefits:**

- ขยายข้อมูลโดยใช้ข้อมูลมาตรฐาน เช่น ตารางวันที่ (_date dimension table_) ที่เป็นรูปแบบเดียวกัน
- ให้ผู้ใช้แบบ _self-service_ เข้าถึงข้อมูลบางส่วนจาก _data warehouse_ ได้แยกต่างหาก
- ปรับปรุงประสิทธิภาพโดยใช้ _dataflows_ เพื่อดึงข้อมูลครั้งเดียวแล้วใช้ซ้ำ ลดเวลาการรีเฟรชข้อมูลจากแหล่งที่โหลดช้า
- ลดความซับซ้อนของแหล่งข้อมูล โดยเปิดเผยเฉพาะ _dataflows_ ให้กับกลุ่มนักวิเคราะห์จำนวนมาก
- รับรองความสม่ำเสมอและคุณภาพของข้อมูล โดยเปิดโอกาสให้ผู้ใช้สามารถทำความสะอาดและแปลงข้อมูลก่อนโหลดไปยังปลายทาง
- ทำให้การรวมข้อมูลง่ายขึ้น โดยมีอินเทอร์เฟซแบบ _low-code_ สำหรับดึงข้อมูลจากหลายแหล่ง

**Limitations:**

- _Dataflows_ ไม่สามารถใช้แทน _data warehouse_ ได้
- ไม่รองรับการควบคุมสิทธิ์ในระดับแถว (_row-level security_)
- ต้องใช้งานภายใน _Fabric capacity workspace_

