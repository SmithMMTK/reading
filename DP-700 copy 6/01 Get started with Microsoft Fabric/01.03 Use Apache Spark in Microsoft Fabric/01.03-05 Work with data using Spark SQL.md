
_API_ ของ _dataframe_ เป็นส่วนหนึ่งของไลบรารี _Spark SQL_ ซึ่งช่วยให้นักวิเคราะห์ข้อมูลสามารถใช้คำสั่งแบบ _SQL expressions_ เพื่อ _query_ และ _manipulate_ ข้อมูลได้ง่ายขึ้น

## Creating database objects in the Spark catalog

_catalog_ ของ _Spark_ คือพื้นที่เก็บข้อมูลเมตา (_metastore_) สำหรับวัตถุข้อมูลเชิงสัมพันธ์ เช่น _views_ และ _tables_ โดยที่ระบบ _Spark runtime_ สามารถใช้ _catalog_ นี้เพื่อผสานโค้ดที่เขียนด้วยภาษาต่าง ๆ ที่ _Spark_ รองรับ ให้ทำงานร่วมกับ _SQL expressions_ ได้อย่างลื่นไหล ซึ่งเหมาะกับนักวิเคราะห์หรือผู้พัฒนาบางคนที่ถนัดใช้ _SQL_

วิธีง่ายที่สุดวิธีหนึ่งในการทำให้ข้อมูลใน _dataframe_ สามารถ _query_ ได้ผ่าน _Spark catalog_ คือการสร้าง _temporary view_ ตามตัวอย่างโค้ดด้านล่าง:

```python
df.createOrReplaceTempView("products_view")
```

_view_ เป็นแบบชั่วคราว หมายความว่า _view_ นี้จะถูกลบโดยอัตโนมัติเมื่อสิ้นสุดเซสชันปัจจุบัน อย่างไรก็ตาม เรายังสามารถสร้าง _tables_ ที่เก็บถาวรไว้ใน _catalog_ ได้ ซึ่งใช้สำหรับกำหนดฐานข้อมูลที่สามารถ _query_ ได้ผ่าน _Spark SQL_

_table_ เป็นโครงสร้างเมตาดาทาที่มีหน้าที่เก็บข้อมูลจริงไว้ในตำแหน่งจัดเก็บข้อมูลที่เชื่อมโยงกับ _catalog_ สำหรับใน Microsoft Fabric ข้อมูลของ _managed_ tables จะถูกเก็บไว้ในตำแหน่งที่ชื่อว่า **Tables** ใน _data lake_ และทุก _table_ ที่สร้างผ่าน _Spark_ จะถูกแสดงไว้ในที่เดียวกันนี้


เราสามารถสร้าง _empty table_ ได้โดยใช้เมธอด spark.catalog.createTable หรือจะบันทึก _dataframe_ เป็น _table_ โดยใช้เมธอด saveAsTable ก็ได้ หากลบ _managed table_ ระบบจะลบข้อมูลจริงที่อยู่เบื้องหลังไปด้วย

ตัวอย่างเช่น โค้ดด้านล่างนี้ใช้บันทึก _dataframe_ เป็น _table_ ใหม่ชื่อว่า **products**:

```
df.write.format("delta").saveAsTable("products")
```

_หมายเหตุ_: _Spark catalog_ รองรับ _tables_ ที่อิงจากไฟล์ในหลายรูปแบบ โดยรูปแบบที่แนะนำสำหรับ Microsoft Fabric คือ **delta** ซึ่งเป็นฟอร์แมตสำหรับเทคโนโลยีข้อมูลเชิงสัมพันธ์บน _Spark_ ที่ชื่อว่า _Delta Lake_ โดย _delta tables_ รองรับฟีเจอร์ที่พบได้ในระบบฐานข้อมูลทั่วไป เช่น _transactions_, _versioning_ และการรองรับข้อมูลแบบ _streaming_

นอกจากนี้ยังสามารถสร้าง _external tables_ ได้ด้วยเมธอด spark.catalog.createExternalTable ซึ่งจะกำหนดเมตาดาไว้ใน _catalog_ แต่ข้อมูลจริงจะอยู่ในตำแหน่งจัดเก็บภายนอก เช่น โฟลเดอร์ในพื้นที่ **Files** ของ _lakehouse_ และการลบ _external table_ จะไม่ลบข้อมูลจริงที่เก็บไว้นั้น

_เคล็ดลับ_: คุณสามารถใช้เทคนิคการ _partitioning_ แบบเดียวกับที่ใช้กับไฟล์ _parquet_ กับ _delta lake tables_ ได้ด้วย ซึ่งช่วยเพิ่ม _performance_ ในการ _query_ ข้อมูลได้อย่างมีประสิทธิภาพมากขึ้น

## Using the Spark SQL API to query data

คุณสามารถใช้ _Spark SQL API_ ในโค้ดที่เขียนด้วยภาษาใดก็ได้ เพื่อ _query_ ข้อมูลจาก _catalog_ ได้ ตัวอย่างเช่น โค้ด _PySpark_ ด้านล่างนี้ใช้คำสั่ง _SQL_ เพื่อดึงข้อมูลจาก _table_ ชื่อ **products** และส่งผลลัพธ์ออกมาในรูปแบบ _dataframe_

```
bikes_df = spark.sql("SELECT ProductID, ProductName, ListPrice \
                      FROM products \
                      WHERE Category IN ('Mountain Bikes', 'Road Bikes')")
display(bikes_df)
```

ผลลัพธ์จากตัวอย่างโค้ดจะมีลักษณะคล้ายกับตารางต่อไปนี้:

|ProductID|ProductName|ListPrice|
|---|---|---|
|771|Mountain-100 Silver, 38|3399.9900|
|839|Road-750 Black, 52|539.9900|
|...|...|...|

## Using SQL code


ตัวอย่างก่อนหน้านี้แสดงให้เห็นวิธีใช้ _Spark SQL API_ เพื่อฝังคำสั่ง _SQL expressions_ ไว้ในโค้ด _Spark_
 
ถ้าใช้งานใน _notebook_ คุณยังสามารถใช้คำสั่งพิเศษ `%%sql` (_magic command_) เพื่อรันโค้ด _SQL_ โดยตรงสำหรับ _query_ ข้อมูลจากวัตถุใน _catalog_ ได้ เช่นตัวอย่างนี้:

```sql
%%sql

SELECT Category, COUNT(ProductID) AS ProductCount
FROM products
GROUP BY Category
ORDER BY Category
```

ตัวอย่างโค้ด _SQL_ นี้จะส่งคืน _resultset_ ที่แสดงเป็นตารางโดยอัตโนมัติใน _notebook_ ดังนี้:

|Category|ProductCount|
|---|---|
|Bib-Shorts|3|
|Bike Racks|1|
|Bike Stands|1|
|...|...|