
[Launch Exercise](https://go.microsoft.com/fwlink/?linkid=2259606)

_data lakehouse_ เป็นแหล่งเก็บข้อมูลเพื่อการวิเคราะห์ที่พบได้ทั่วไปในระบบวิเคราะห์ข้อมูลขนาดใหญ่บนคลาวด์ หนึ่งในหน้าที่หลักของ _data engineer_ คือการออกแบบและดูแลกระบวนการ _ingestion_ ข้อมูลจากแหล่งข้อมูลปฏิบัติการ (_operational data sources_) ที่หลากหลายเข้าสู่ _lakehouse_

ใน _Microsoft Fabric_ คุณสามารถสร้างกระบวนการ _ETL (extract, transform, and load)_ หรือ _ELT (extract, load, and transform)_ สำหรับการนำเข้าข้อมูลผ่านการสร้าง _pipelines_

_Fabric_ ยังรองรับ _Apache Spark_ ทำให้สามารถเขียนและรันโค้ดเพื่อประมวลผลข้อมูลขนาดใหญ่ได้ การผสานการทำงานของ _pipeline_ และ _Spark_ ใน _Fabric_ ช่วยให้สามารถออกแบบกระบวนการ _ingestion_ ที่ซับซ้อนได้ เช่น การคัดลอกข้อมูลจากแหล่งภายนอกเข้าสู่ _OneLake storage_ ซึ่งเป็นฐานของ _lakehouse_ แล้วใช้ _Spark code_ เพื่อแปลงข้อมูลในแบบที่ต้องการก่อนโหลดเข้าสู่ _table_ เพื่อใช้ในการวิเคราะห์ต่อไป

